{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "178bd80c-1b50-4644-a010-899a6a0d163d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.26.4 in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 2)) (2.3.0)\n",
      "Requirement already satisfied: scipy<1.14.0 in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 3)) (1.13.1)\n",
      "Requirement already satisfied: xgboost in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 4)) (3.0.2)\n",
      "Requirement already satisfied: lightgbm in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 5)) (4.6.0)\n",
      "Requirement already satisfied: catboost in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 6)) (1.2.8)\n",
      "Requirement already satisfied: tensorflow in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 7)) (2.19.0)\n",
      "Requirement already satisfied: keras in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 8)) (3.10.0)\n",
      "Requirement already satisfied: torch in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 9)) (2.7.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 10)) (0.22.1+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 11)) (2.7.1+cu118)\n",
      "Requirement already satisfied: spacy in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 12)) (3.8.7)\n",
      "Requirement already satisfied: nltk in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 13)) (3.9.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 14)) (4.12.3)\n",
      "Requirement already satisfied: requests in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 15)) (2.32.4)\n",
      "Requirement already satisfied: selenium in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 16)) (4.34.2)\n",
      "Requirement already satisfied: undetected-chromedriver in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 17)) (3.5.5)\n",
      "Requirement already satisfied: mesa==1.2.1 in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 18)) (1.2.1)\n",
      "Requirement already satisfied: deap in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 19)) (1.4.3)\n",
      "Requirement already satisfied: matplotlib in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 20)) (3.10.0)\n",
      "Requirement already satisfied: seaborn in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 21)) (0.13.2)\n",
      "Requirement already satisfied: plotly in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 22)) (6.0.1)\n",
      "Requirement already satisfied: streamlit in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 23)) (1.46.0)\n",
      "Requirement already satisfied: gradio in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 24)) (5.34.0)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 25)) (4.67.1)\n",
      "Requirement already satisfied: loguru in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 26)) (0.7.3)\n",
      "Requirement already satisfied: pyyaml in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 27)) (6.0.2)\n",
      "Requirement already satisfied: networkx in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 28)) (3.4.2)\n",
      "Requirement already satisfied: pyro-ppl in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 29)) (1.9.1)\n",
      "Requirement already satisfied: pymc in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 30)) (5.25.1)\n",
      "Requirement already satisfied: sentence-transformers in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 31)) (4.1.0)\n",
      "Requirement already satisfied: openai in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 32)) (1.97.1)\n",
      "Requirement already satisfied: transformers in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 33)) (4.48.2)\n",
      "Requirement already satisfied: accelerate in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 34)) (1.8.1)\n",
      "Requirement already satisfied: datasets in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 35)) (4.0.0)\n",
      "Requirement already satisfied: fsspec in c:\\anaconda3\\lib\\site-packages (from -r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 36)) (2025.3.0)\n",
      "Requirement already satisfied: click in c:\\anaconda3\\lib\\site-packages (from mesa==1.2.1->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 18)) (8.1.8)\n",
      "Requirement already satisfied: cookiecutter in c:\\anaconda3\\lib\\site-packages (from mesa==1.2.1->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 18)) (2.6.0)\n",
      "Requirement already satisfied: tornado in c:\\anaconda3\\lib\\site-packages (from mesa==1.2.1->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 18)) (6.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\anaconda3\\lib\\site-packages (from pandas->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\anaconda3\\lib\\site-packages (from pandas->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\anaconda3\\lib\\site-packages (from pandas->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: graphviz in c:\\anaconda3\\lib\\site-packages (from catboost->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 6)) (0.21)\n",
      "Requirement already satisfied: six in c:\\anaconda3\\lib\\site-packages (from catboost->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 6)) (1.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\anaconda3\\lib\\site-packages (from tensorflow->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 7)) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\anaconda3\\lib\\site-packages (from tensorflow->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 7)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\anaconda3\\lib\\site-packages (from tensorflow->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 7)) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\anaconda3\\lib\\site-packages (from tensorflow->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 7)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\anaconda3\\lib\\site-packages (from tensorflow->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\anaconda3\\lib\\site-packages (from tensorflow->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 7)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\anaconda3\\lib\\site-packages (from tensorflow->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 7)) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\anaconda3\\lib\\site-packages (from tensorflow->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 7)) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\anaconda3\\lib\\site-packages (from tensorflow->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 7)) (5.29.3)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda3\\lib\\site-packages (from tensorflow->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 7)) (78.1.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\anaconda3\\lib\\site-packages (from tensorflow->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 7)) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\anaconda3\\lib\\site-packages (from tensorflow->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 7)) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\anaconda3\\lib\\site-packages (from tensorflow->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 7)) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\anaconda3\\lib\\site-packages (from tensorflow->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 7)) (1.73.1)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\anaconda3\\lib\\site-packages (from tensorflow->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 7)) (2.19.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\anaconda3\\lib\\site-packages (from tensorflow->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 7)) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\anaconda3\\lib\\site-packages (from tensorflow->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 7)) (0.5.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\anaconda3\\lib\\site-packages (from requests->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 15)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda3\\lib\\site-packages (from requests->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 15)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\anaconda3\\lib\\site-packages (from requests->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 15)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\lib\\site-packages (from requests->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 15)) (2025.7.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 7)) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 7)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 7)) (3.1.3)\n",
      "Requirement already satisfied: rich in c:\\anaconda3\\lib\\site-packages (from keras->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 8)) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\anaconda3\\lib\\site-packages (from keras->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 8)) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\anaconda3\\lib\\site-packages (from keras->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 8)) (0.16.0)\n",
      "Requirement already satisfied: filelock in c:\\anaconda3\\lib\\site-packages (from torch->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 9)) (3.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\anaconda3\\lib\\site-packages (from torch->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 9)) (1.13.3)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda3\\lib\\site-packages (from torch->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 9)) (3.1.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\anaconda3\\lib\\site-packages (from torchvision->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 10)) (11.1.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\anaconda3\\lib\\site-packages (from spacy->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 12)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\anaconda3\\lib\\site-packages (from spacy->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 12)) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\anaconda3\\lib\\site-packages (from spacy->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 12)) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\anaconda3\\lib\\site-packages (from spacy->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 12)) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\anaconda3\\lib\\site-packages (from spacy->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 12)) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\anaconda3\\lib\\site-packages (from spacy->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 12)) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\anaconda3\\lib\\site-packages (from spacy->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 12)) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\anaconda3\\lib\\site-packages (from spacy->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 12)) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\anaconda3\\lib\\site-packages (from spacy->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 12)) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\anaconda3\\lib\\site-packages (from spacy->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 12)) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\anaconda3\\lib\\site-packages (from spacy->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 12)) (0.16.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\anaconda3\\lib\\site-packages (from spacy->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 12)) (2.10.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\anaconda3\\lib\\site-packages (from spacy->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 12)) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\anaconda3\\lib\\site-packages (from tqdm->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 25)) (0.4.6)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 12)) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 12)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 12)) (2.27.1)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 12)) (1.2.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 12)) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 12)) (1.5.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 12)) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 12)) (5.2.1)\n",
      "Requirement already satisfied: joblib in c:\\anaconda3\\lib\\site-packages (from nltk->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 13)) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\anaconda3\\lib\\site-packages (from nltk->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 13)) (2024.11.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\anaconda3\\lib\\site-packages (from beautifulsoup4->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 14)) (2.5)\n",
      "Requirement already satisfied: trio~=0.30.0 in c:\\anaconda3\\lib\\site-packages (from selenium->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 16)) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.12.2 in c:\\anaconda3\\lib\\site-packages (from selenium->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 16)) (0.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in c:\\anaconda3\\lib\\site-packages (from selenium->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 16)) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 16)) (24.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 16)) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 16)) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 16)) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 16)) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\anaconda3\\lib\\site-packages (from trio-websocket~=0.12.2->selenium->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 16)) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\anaconda3\\lib\\site-packages (from urllib3[socks]~=2.5.0->selenium->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 16)) (1.7.1)\n",
      "Requirement already satisfied: websockets in c:\\anaconda3\\lib\\site-packages (from undetected-chromedriver->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 17)) (15.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\anaconda3\\lib\\site-packages (from matplotlib->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 20)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\anaconda3\\lib\\site-packages (from matplotlib->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 20)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\anaconda3\\lib\\site-packages (from matplotlib->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 20)) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\anaconda3\\lib\\site-packages (from matplotlib->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 20)) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\anaconda3\\lib\\site-packages (from matplotlib->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 20)) (3.2.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\anaconda3\\lib\\site-packages (from plotly->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 22)) (1.31.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\anaconda3\\lib\\site-packages (from streamlit->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 23)) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in c:\\anaconda3\\lib\\site-packages (from streamlit->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 23)) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in c:\\anaconda3\\lib\\site-packages (from streamlit->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 23)) (5.5.1)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\anaconda3\\lib\\site-packages (from streamlit->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 23)) (19.0.0)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\anaconda3\\lib\\site-packages (from streamlit->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 23)) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\anaconda3\\lib\\site-packages (from streamlit->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 23)) (0.10.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\anaconda3\\lib\\site-packages (from streamlit->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 23)) (4.0.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\anaconda3\\lib\\site-packages (from streamlit->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 23)) (3.1.43)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 23)) (4.23.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 23)) (4.0.7)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 23)) (4.0.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\anaconda3\\lib\\site-packages (from gradio->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 24)) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\anaconda3\\lib\\site-packages (from gradio->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 24)) (4.7.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\anaconda3\\lib\\site-packages (from gradio->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 24)) (0.115.13)\n",
      "Requirement already satisfied: ffmpy in c:\\anaconda3\\lib\\site-packages (from gradio->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 24)) (0.6.0)\n",
      "Requirement already satisfied: gradio-client==1.10.3 in c:\\anaconda3\\lib\\site-packages (from gradio->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 24)) (1.10.3)\n",
      "Requirement already satisfied: groovy~=0.1 in c:\\anaconda3\\lib\\site-packages (from gradio->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 24)) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\anaconda3\\lib\\site-packages (from gradio->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 24)) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in c:\\anaconda3\\lib\\site-packages (from gradio->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 24)) (0.32.3)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\anaconda3\\lib\\site-packages (from gradio->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 24)) (3.0.2)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\anaconda3\\lib\\site-packages (from gradio->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 24)) (3.10.18)\n",
      "Requirement already satisfied: pydub in c:\\anaconda3\\lib\\site-packages (from gradio->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 24)) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\anaconda3\\lib\\site-packages (from gradio->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 24)) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in c:\\anaconda3\\lib\\site-packages (from gradio->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 24)) (0.12.0)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\anaconda3\\lib\\site-packages (from gradio->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 24)) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\anaconda3\\lib\\site-packages (from gradio->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 24)) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\anaconda3\\lib\\site-packages (from gradio->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 24)) (0.46.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\anaconda3\\lib\\site-packages (from gradio->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 24)) (0.13.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\anaconda3\\lib\\site-packages (from gradio->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 24)) (0.34.3)\n",
      "Requirement already satisfied: win32-setctime>=1.0.0 in c:\\anaconda3\\lib\\site-packages (from loguru->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 26)) (1.2.0)\n",
      "Requirement already satisfied: pyro-api>=0.1.1 in c:\\anaconda3\\lib\\site-packages (from pyro-ppl->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 29)) (0.1.2)\n",
      "Requirement already satisfied: arviz>=0.13.0 in c:\\anaconda3\\lib\\site-packages (from pymc->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 30)) (0.22.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\anaconda3\\lib\\site-packages (from pymc->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 30)) (3.0.0)\n",
      "Requirement already satisfied: pytensor<2.32,>=2.31.7 in c:\\anaconda3\\lib\\site-packages (from pymc->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 30)) (2.31.7)\n",
      "Requirement already satisfied: threadpoolctl<4.0.0,>=3.1.0 in c:\\anaconda3\\lib\\site-packages (from pymc->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 30)) (3.5.0)\n",
      "Requirement already satisfied: etuples in c:\\anaconda3\\lib\\site-packages (from pytensor<2.32,>=2.31.7->pymc->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 30)) (0.3.10)\n",
      "Requirement already satisfied: logical-unification in c:\\anaconda3\\lib\\site-packages (from pytensor<2.32,>=2.31.7->pymc->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 30)) (0.4.6)\n",
      "Requirement already satisfied: miniKanren in c:\\anaconda3\\lib\\site-packages (from pytensor<2.32,>=2.31.7->pymc->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 30)) (1.0.5)\n",
      "Requirement already satisfied: cons in c:\\anaconda3\\lib\\site-packages (from pytensor<2.32,>=2.31.7->pymc->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 30)) (0.4.7)\n",
      "Requirement already satisfied: scikit-learn in c:\\anaconda3\\lib\\site-packages (from sentence-transformers->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 31)) (1.6.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\anaconda3\\lib\\site-packages (from transformers->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 33)) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\anaconda3\\lib\\site-packages (from transformers->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 33)) (0.5.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\anaconda3\\lib\\site-packages (from openai->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 32)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\anaconda3\\lib\\site-packages (from openai->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 32)) (0.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 24)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 24)) (0.16.0)\n",
      "Requirement already satisfied: psutil in c:\\anaconda3\\lib\\site-packages (from accelerate->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 34)) (5.9.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\anaconda3\\lib\\site-packages (from datasets->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 35)) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\anaconda3\\lib\\site-packages (from datasets->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 35)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\anaconda3\\lib\\site-packages (from datasets->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 35)) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 35)) (3.11.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 35)) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 35)) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 35)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 35)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 35)) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 35)) (1.18.0)\n",
      "Requirement already satisfied: xarray>=2023.7.0 in c:\\anaconda3\\lib\\site-packages (from arviz>=0.13.0->pymc->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 30)) (2025.4.0)\n",
      "Requirement already satisfied: h5netcdf>=1.0.2 in c:\\anaconda3\\lib\\site-packages (from arviz>=0.13.0->pymc->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 30)) (1.6.3)\n",
      "Requirement already satisfied: xarray-einstats>=0.3 in c:\\anaconda3\\lib\\site-packages (from arviz>=0.13.0->pymc->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 30)) (0.9.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 7)) (0.45.1)\n",
      "Requirement already satisfied: pycparser in c:\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.30.0->selenium->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 16)) (2.21)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 23)) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 23)) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 23)) (0.22.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 12)) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\anaconda3\\lib\\site-packages (from rich->keras->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 8)) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\anaconda3\\lib\\site-packages (from rich->keras->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 8)) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 8)) (0.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 9)) (1.3.0)\n",
      "Requirement already satisfied: toolz in c:\\anaconda3\\lib\\site-packages (from logical-unification->pytensor<2.32,>=2.31.7->pymc->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 30)) (1.0.0)\n",
      "Requirement already satisfied: multipledispatch in c:\\anaconda3\\lib\\site-packages (from logical-unification->pytensor<2.32,>=2.31.7->pymc->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 30)) (0.6.0)\n",
      "Requirement already satisfied: binaryornot>=0.4.4 in c:\\anaconda3\\lib\\site-packages (from cookiecutter->mesa==1.2.1->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 18)) (0.4.4)\n",
      "Requirement already satisfied: python-slugify>=4.0.0 in c:\\anaconda3\\lib\\site-packages (from cookiecutter->mesa==1.2.1->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 18)) (5.0.2)\n",
      "Requirement already satisfied: arrow in c:\\anaconda3\\lib\\site-packages (from cookiecutter->mesa==1.2.1->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 18)) (1.3.0)\n",
      "Requirement already satisfied: chardet>=3.0.2 in c:\\anaconda3\\lib\\site-packages (from binaryornot>=0.4.4->cookiecutter->mesa==1.2.1->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 18)) (4.0.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\anaconda3\\lib\\site-packages (from python-slugify>=4.0.0->cookiecutter->mesa==1.2.1->-r C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt (line 18)) (1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r \"C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49484891-83f9-4e0d-8b67-92ec7c88512d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\anaconda3\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy in c:\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda3\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: faker in c:\\anaconda3\\lib\\site-packages (37.5.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: requests in c:\\anaconda3\\lib\\site-packages (2.32.4)\n",
      "Requirement already satisfied: pyarrow in c:\\anaconda3\\lib\\site-packages (19.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\anaconda3\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\anaconda3\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\lib\\site-packages (from requests) (2025.7.14)\n",
      "Requirement already satisfied: six>=1.5 in c:\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy scikit-learn tqdm faker beautifulsoup4 requests pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88f89172-82b5-4e26-89ec-e5ccaf90b533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, random, warnings, sys, time, pickle, urllib.parse, datetime as dt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fc8a3bc-42ef-446f-a1ea-c3e2f6950c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SELENIUM = False\n",
    "if USE_SELENIUM:\n",
    "    try:\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "    except Exception:\n",
    "        print(\"[WARN] Selenium not available. Set USE_SELENIUM=False or install it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24e90c09-fd66-4e35-9098-aeae4d857dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = r\"C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\datasets\"\n",
    "OUT_ROOT  = r\"C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\"\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33547f03-a139-4c1d-bd28-3fbcb2376e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITE_PARQUET = False   \n",
    "USE_OHE = False        \n",
    "SAVE_ENCODERS = True    \n",
    "SYNTHETIC_N  = 50000\n",
    "RF_WARM_BATCH = 5000   \n",
    "PRED_THRESH   = 0.30\n",
    "RAND_SEED     = 42\n",
    "N_AGENTS = 20\n",
    "N_GENERATIONS = 10\n",
    "TOP_K = 6\n",
    "N_CHILDREN = N_AGENTS - TOP_K\n",
    "AGENT_BATCH = 5000\n",
    "MUTATION_RATE = 0.2\n",
    "GA_REAL_RATIO = 0.7    \n",
    "N_META = 20000\n",
    "np.random.seed(RAND_SEED)\n",
    "random.seed(RAND_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "115c11cb-0530-4604-9498-2719f9501f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_org_key(df, col):\n",
    "    return df[col].astype(str).str.lower().str.strip()\n",
    "def safe_read_csv(path, **kw):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"[WARN] Missing: {path}\"); return pd.DataFrame()\n",
    "    try:\n",
    "        return pd.read_csv(path, **kw)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] CSV read fail {path}: {e}\"); return pd.DataFrame()\n",
    "def safe_read_excel(path, **kw):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"[WARN] Missing: {path}\"); return pd.DataFrame()\n",
    "    try:\n",
    "        return pd.read_excel(path, **kw)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Excel read fail {path}: {e}\"); return pd.DataFrame()\n",
    "def normalize_list_like_series(s: pd.Series) -> pd.Series:\n",
    "    return (\n",
    "        s.astype(str)\n",
    "         .str.replace(r'[\\[\\]\\']', '', regex=True)\n",
    "         .str.replace(r'\\s+', ' ', regex=True)\n",
    "         .str.strip()\n",
    "         .replace({'nan': np.nan, 'None': np.nan, '': np.nan})\n",
    "    )\n",
    "def to_io(df, path_stem):\n",
    "    if WRITE_PARQUET:\n",
    "        p = os.path.join(OUT_ROOT, path_stem + \".parquet\")\n",
    "        df.to_parquet(p, index=False)\n",
    "    else:\n",
    "        p = os.path.join(OUT_ROOT, path_stem + \".csv\")\n",
    "        df.to_csv(p, index=False)\n",
    "    return p\n",
    "def from_io(path_stem):\n",
    "    p_parq = os.path.join(OUT_ROOT, path_stem + \".parquet\")\n",
    "    p_csv  = os.path.join(OUT_ROOT, path_stem + \".csv\")\n",
    "    if os.path.exists(p_parq):\n",
    "        return pd.read_parquet(p_parq)\n",
    "    return pd.read_csv(p_csv, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7fbf608-4dda-47ff-bd6b-361f29551197",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROUND_TO_STAGE = {\n",
    "    'angel': 'Seed',\n",
    "    'seed': 'Seed',\n",
    "    'pre-seed': 'Seed',\n",
    "    'series_a': 'Early',\n",
    "    'a': 'Early',\n",
    "    'series_b': 'Growth',\n",
    "    'b': 'Growth',\n",
    "    'series_c': 'Growth',\n",
    "    'c': 'Growth',\n",
    "    'mezzanine': 'Late',\n",
    "    'late': 'Late',\n",
    "    'private_equity': 'Late',\n",
    "    'ipo': 'Exit',\n",
    "    'acquired': 'Exit'\n",
    "}\n",
    "def map_company_stage(latest_round_type: pd.Series) -> pd.Series:\n",
    "    def _m(x):\n",
    "        if pd.isna(x): return \"unknown\"\n",
    "        s = str(x).strip().lower().replace(\" \", \"_\")\n",
    "        return ROUND_TO_STAGE.get(s, \"unknown\")\n",
    "    return latest_round_type.apply(_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3e8d7ab-e308-4a5c-b764-dc9853f20f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_news_features_for_company(name, days_recent=30, max_items=200):\n",
    "    \"\"\"Return (news_count_all, latest_news_date, news_count_recent) using Google News RSS.\"\"\"\n",
    "    if not isinstance(name, str) or not name.strip():\n",
    "        return 0, pd.NaT, 0\n",
    "    q = urllib.parse.quote_plus(name.strip())\n",
    "    url = f\"https://news.google.com/rss/search?q={q}&hl=en-US&gl=US&ceid=US:en\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        if r.status_code != 200:\n",
    "            return 0, pd.NaT, 0\n",
    "        soup = BeautifulSoup(r.content, \"xml\")\n",
    "        items = soup.find_all(\"item\")\n",
    "        total = min(len(items), max_items)\n",
    "        latest_date = pd.NaT\n",
    "        recent_count = 0\n",
    "        cutoff = pd.Timestamp.utcnow() - pd.Timedelta(days=days_recent)\n",
    "        for it in items[:max_items]:\n",
    "            pub = it.find(\"pubDate\")\n",
    "            if pub and pub.text:\n",
    "                try:\n",
    "                    dt_parsed = pd.to_datetime(pub.text, errors='coerce', utc=True)\n",
    "                except Exception:\n",
    "                    dt_parsed = pd.NaT\n",
    "                if pd.notna(dt_parsed):\n",
    "                    if pd.isna(latest_date) or dt_parsed > latest_date:\n",
    "                        latest_date = dt_parsed\n",
    "                    if dt_parsed >= cutoff:\n",
    "                        recent_count += 1\n",
    "        return total, latest_date, recent_count\n",
    "    except Exception:\n",
    "        return 0, pd.NaT, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dee0aa74-9d97-4e01-8b8f-a4d2c2cee9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selenium_fetch_title(url):\n",
    "    if not USE_SELENIUM:\n",
    "        return None\n",
    "    try:\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless=new\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(url)\n",
    "        time.sleep(2)\n",
    "        title = driver.title\n",
    "        driver.quit()\n",
    "        return title\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28e52bca-bdac-4a58-99b5-83e6724b6f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "acq   = safe_read_csv(os.path.join(DATA_ROOT, \"acquisitions.csv\"))\n",
    "deg   = safe_read_csv(os.path.join(DATA_ROOT, \"degrees.csv\"))\n",
    "funds = safe_read_csv(os.path.join(DATA_ROOT, \"funds.csv\"))\n",
    "fr    = safe_read_csv(os.path.join(DATA_ROOT, \"funding_rounds.csv\"))\n",
    "inv   = safe_read_csv(os.path.join(DATA_ROOT, \"investments.csv\"))\n",
    "ipos  = safe_read_csv(os.path.join(DATA_ROOT, \"ipos.csv\"))\n",
    "mile  = safe_read_csv(os.path.join(DATA_ROOT, \"milestones.csv\"))\n",
    "obj   = safe_read_csv(os.path.join(DATA_ROOT, \"objects.csv\"), dtype=str)\n",
    "off   = safe_read_csv(os.path.join(DATA_ROOT, \"offices.csv\"))\n",
    "people= safe_read_csv(os.path.join(DATA_ROOT, \"people.csv\"))\n",
    "rel   = safe_read_csv(os.path.join(DATA_ROOT, \"relationships.csv\"))\n",
    "yc    = safe_read_csv(os.path.join(DATA_ROOT, \"yc_companies.csv\"))\n",
    "invvc = safe_read_csv(os.path.join(DATA_ROOT, \"investments_VC.csv\"), encoding='latin1')\n",
    "yc_clean = safe_read_excel(os.path.join(DATA_ROOT, \"yc_cleaned_data.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89ee8f4e-3544-45f6-827d-944118ec5419",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not obj.empty and 'id' in obj.columns: obj['org_key'] = make_org_key(obj, 'id')\n",
    "if not fr.empty and 'object_id' in fr.columns: fr['org_key'] = make_org_key(fr, 'object_id')\n",
    "if not acq.empty and 'acquired_object_id' in acq.columns: acq['org_key'] = make_org_key(acq, 'acquired_object_id')\n",
    "if not ipos.empty and 'object_id' in ipos.columns: ipos['org_key'] = make_org_key(ipos, 'object_id')\n",
    "if not off.empty and 'object_id' in off.columns: off['org_key'] = make_org_key(off, 'object_id')\n",
    "if not mile.empty and 'object_id' in mile.columns: mile['org_key'] = make_org_key(mile, 'object_id')\n",
    "if not people.empty and 'object_id' in people.columns: people['org_key'] = make_org_key(people, 'object_id')\n",
    "if not rel.empty and 'relationship_object_id' in rel.columns: rel['org_key'] = make_org_key(rel, 'relationship_object_id')\n",
    "if not inv.empty and 'funded_object_id' in inv.columns: inv['org_key'] = make_org_key(inv, 'funded_object_id')\n",
    "if not yc_clean.empty:\n",
    "    if 'permalink' in yc_clean.columns:\n",
    "        yc_clean['org_key'] = make_org_key(yc_clean, 'permalink')\n",
    "    elif 'name' in yc_clean.columns:\n",
    "        yc_clean['org_key'] = yc_clean['name'].astype(str).str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d3c9fc6-af57-4098-b216-99d011566cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not off.empty:\n",
    "    if 'description' in off.columns:\n",
    "        hq = (off[off['description'].astype(str).str.lower().str.contains('head', na=False)]\n",
    "              .groupby('org_key').first().reset_index())\n",
    "        if hq.empty: hq = off.groupby('org_key').first().reset_index()\n",
    "    else:\n",
    "        hq = off.groupby('org_key').first().reset_index()\n",
    "    hq = hq.rename(columns={'city': 'hq_city', 'state_code': 'hq_state', 'country_code': 'hq_country'})\n",
    "else:\n",
    "    hq = pd.DataFrame(columns=['org_key', 'hq_city', 'hq_state', 'hq_country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cdbed8f-f1a0-4cd0-ad62-a4722cadeb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not fr.empty:\n",
    "    fr['funded_at'] = pd.to_datetime(fr.get('funded_at', pd.NaT), errors='coerce')\n",
    "    fr = fr.sort_values(['org_key', 'funded_at'])\n",
    "    funding = fr.groupby('org_key').agg(\n",
    "        total_raised_usd=('raised_amount_usd', 'sum'),\n",
    "        num_rounds=('funding_round_code', 'count'),\n",
    "        latest_round_type=('funding_round_code', 'last'),\n",
    "        first_funding_at=('funded_at', 'min'),\n",
    "        last_funding_at=('funded_at', 'max'),\n",
    "        avg_round_size_usd=('raised_amount_usd', 'mean'),\n",
    "        funding_round_codes=('funding_round_code', lambda x: ','.join(sorted(set(x.dropna().astype(str)))))\n",
    "    ).reset_index()\n",
    "    dur_years = ((funding['last_funding_at'] - funding['first_funding_at']).dt.days / 365.25).replace(0, np.nan)\n",
    "    funding['funding_per_year'] = funding['total_raised_usd'] / dur_years\n",
    "    funding['company_stage'] = map_company_stage(funding['latest_round_type'])\n",
    "else:\n",
    "    funding = pd.DataFrame(columns=[\n",
    "        'org_key', 'total_raised_usd', 'num_rounds', 'latest_round_type',\n",
    "        'first_funding_at', 'last_funding_at', 'avg_round_size_usd',\n",
    "        'funding_round_codes', 'funding_per_year', 'company_stage'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ae81b80-ff55-42c8-b884-f393b88c3fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not inv.empty and 'investor_object_id' in inv.columns:\n",
    "    investors_count = inv.groupby('org_key')['investor_object_id'].nunique().reset_index().rename(columns={'investor_object_id': 'investors_count'})\n",
    "else:\n",
    "    investors_count = pd.DataFrame(columns=['org_key', 'investors_count'])\n",
    "if not inv.empty:\n",
    "    if 'is_lead_investor' in inv.columns:\n",
    "        lead_investors = (inv[inv['is_lead_investor'] == 1]\n",
    "                          .groupby('org_key')['investor_object_id']\n",
    "                          .nunique().rename('lead_investors').reset_index())\n",
    "    else:\n",
    "        lead_investors = (inv.groupby('org_key')['investor_object_id']\n",
    "                          .nunique().rename('lead_investors').reset_index())\n",
    "else:\n",
    "    lead_investors = pd.DataFrame(columns=['org_key', 'lead_investors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "885bc9e9-2a6f-46e4-9f1b-1a18a8922cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit_flag = pd.DataFrame(columns=['org_key', 'exit_flag'])\n",
    "ipo_at = pd.DataFrame(columns=['org_key', 'ipo_at'])\n",
    "acquired_at = pd.DataFrame(columns=['org_key', 'acquired_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff58f990-7098-41b8-8beb-6d5dcc2a8073",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not acq.empty:\n",
    "    acquired_at = acq[['org_key', 'acquired_at']].dropna(subset=['org_key']).drop_duplicates()\n",
    "if not ipos.empty:\n",
    "    ipo_at = ipos[['org_key', 'public_at']].dropna(subset=['org_key']).drop_duplicates().rename(columns={'public_at': 'ipo_at'})\n",
    "if (not acq.empty) or (not ipos.empty):\n",
    "    ef = pd.concat([\n",
    "        acq[['org_key']] if 'org_key' in acq else pd.DataFrame(columns=['org_key']),\n",
    "        ipos[['org_key']] if 'org_key' in ipos else pd.DataFrame(columns=['org_key'])\n",
    "    ], ignore_index=True).drop_duplicates()\n",
    "    ef['exit_flag'] = 1\n",
    "    exit_flag = ef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4233614-d5cd-4b5e-9502-5076a495ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not mile.empty:\n",
    "    milestones_count = mile.groupby('org_key').size().rename('milestones_count').reset_index()\n",
    "    milestone_latest = mile.groupby('org_key')['milestone_at'].max().reset_index().rename(columns={'milestone_at': 'milestone_latest'})\n",
    "else:\n",
    "    milestones_count = pd.DataFrame(columns=['org_key', 'milestones_count'])\n",
    "    milestone_latest = pd.DataFrame(columns=['org_key', 'milestone_latest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "835d69a1-8c4c-4458-8cd4-1a08b0d7ab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "founders = pd.DataFrame(columns=['org_key', 'num_founders'])\n",
    "founder_education = pd.DataFrame(columns=['org_key', 'founder_education'])\n",
    "founder_universities = pd.DataFrame(columns=['org_key', 'founder_universities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37ee8032-88e5-4c5e-9b21-f2bd1f0677b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not rel.empty:\n",
    "    founder_rels = rel[rel['title'].astype(str).str.lower().str.contains('founder', na=False)]\n",
    "    if not founder_rels.empty:\n",
    "        founders = (founder_rels.groupby('org_key')['person_object_id']\n",
    "                    .nunique().reset_index().rename(columns={'person_object_id': 'num_founders'}))\n",
    "        if not deg.empty:\n",
    "            deg = deg.rename(columns={'object_id': 'person_key'})\n",
    "            deg_founders = pd.merge(\n",
    "                founder_rels[['org_key', 'person_object_id']],\n",
    "                deg,\n",
    "                left_on='person_object_id',\n",
    "                right_on='person_key',\n",
    "                how='left'\n",
    "            )\n",
    "            founder_education = (deg_founders.groupby('org_key')['degree_type']\n",
    "                                 .apply(lambda x: ','.join(sorted(set([str(v) for v in x.dropna()]))))\n",
    "                                 .reset_index().rename(columns={'degree_type': 'founder_education'}))\n",
    "            founder_universities = (deg_founders.groupby('org_key')['institution']\n",
    "                                    .apply(lambda x: ','.join(sorted(set([str(v) for v in x.dropna()]))))\n",
    "                                    .reset_index().rename(columns={'institution': 'founder_universities'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eaf2a0a3-0972-4990-b493-e475aec5e3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "yc_subset = pd.DataFrame(columns=['org_key'])\n",
    "if not yc_clean.empty and 'org_key' in yc_clean.columns:\n",
    "    keep_cols = [c for c in [\"yc_batch\", \"yc_batch_year\", \"yc_status\", \"yc_top_company\", \"is_unicorn\"] if c in yc_clean.columns]\n",
    "    yc_subset = yc_clean[['org_key'] + keep_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28b44b0c-0a66-4bd8-97d5-a1fc590f999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_counts = pd.DataFrame(columns=['org_key', 'vc_deal_count', 'has_vc_backing'])\n",
    "if not invvc.empty:\n",
    "    invvc['org_key'] = invvc.get('permalink', '').astype(str).str.lower().str.strip()\n",
    "    tmp = invvc.groupby('org_key').size().rename('vc_deal_count').reset_index()\n",
    "    tmp['has_vc_backing'] = 1\n",
    "    vc_counts = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f4b0594-6c19-48c1-9745-202e82cb5369",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "funds_feat = pd.DataFrame(columns=['org_key', 'num_funds', 'funds_total_usd', 'latest_fund_raised_at'])\n",
    "if not funds.empty:\n",
    "    if 'object_id' in funds.columns:\n",
    "        funds['org_key'] = make_org_key(funds, 'object_id')\n",
    "    elif 'permalink' in funds.columns:\n",
    "        funds['org_key'] = make_org_key(funds, 'permalink')\n",
    "    else:\n",
    "        funds['org_key'] = np.nan\n",
    "    amount_candidates = [\n",
    "        'raised_amount_usd', 'raised_amount', 'fund_size_usd', 'fund_size',\n",
    "        'capital_committed_usd', 'capital_committed'\n",
    "    ]\n",
    "    date_candidates = ['raised_at', 'announced_on', 'closed_on', 'created_at']\n",
    "    amount_col = next((c for c in amount_candidates if c in funds.columns), None)\n",
    "    date_col   = next((c for c in date_candidates   if c in funds.columns), None)\n",
    "    if amount_col:\n",
    "        funds[amount_col] = pd.to_numeric(funds[amount_col], errors='coerce')\n",
    "    if date_col:\n",
    "        funds[date_col] = pd.to_datetime(funds[date_col], errors='coerce', utc=True)\n",
    "    agg_dict = {'num_funds': (amount_col if amount_col else (date_col if date_col else 'org_key'), 'count')}\n",
    "    if amount_col:\n",
    "        agg_dict['funds_total_usd'] = (amount_col, 'sum')\n",
    "    if date_col:\n",
    "        agg_dict['latest_fund_raised_at'] = (date_col, 'max')\n",
    "    if 'org_key' in funds.columns:\n",
    "        funds_feat = (\n",
    "            funds.dropna(subset=['org_key'])\n",
    "                 .groupby('org_key')\n",
    "                 .agg(**agg_dict)\n",
    "                 .reset_index()\n",
    "        )\n",
    "    for col in ['num_funds','funds_total_usd','latest_fund_raised_at']:\n",
    "        if col not in funds_feat.columns:\n",
    "            funds_feat[col] = 0 if col != 'latest_fund_raised_at' else pd.NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7758bf0-d4ad-493b-a484-102ea01e8439",
   "metadata": {},
   "outputs": [],
   "source": [
    "if obj.empty:\n",
    "    raise ValueError(\"objects.csv missing/empty; cannot build master.\")\n",
    "obj['founded_year'] = pd.to_datetime(obj.get('founded_at', pd.NaT), errors='coerce').dt.year\n",
    "obj['company_age']  = 2025 - obj['founded_year'].fillna(2025)\n",
    "obj['funding_total_usd'] = pd.to_numeric(obj.get('funding_total_usd', 0), errors='coerce')\n",
    "obj['log_funding']  = np.log10(obj['funding_total_usd'].fillna(0) + 1)\n",
    "main_cols = [\"name\",\"org_key\",\"category_code\",\"status\",\"founded_at\",\"founded_year\",\"company_age\",\"log_funding\",\"funding_total_usd\"]\n",
    "master = obj[[c for c in main_cols if c in obj.columns]].copy()\n",
    "num_offices = (off.groupby('org_key').size().reset_index(name='num_offices')) if not off.empty else pd.DataFrame(columns=['org_key','num_offices'])\n",
    "num_investors = investors_count.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba19074d-282d-4b2b-a59e-898614df7fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NEWS_FETCH = 500  \n",
    "sampled_obj = obj[['org_key','name']].dropna().sample(MAX_NEWS_FETCH, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4190c36-e972-4e2a-81ef-6236280c5383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be4bff00-701e-41bc-8906-83da6561e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_news_features_for_company_cached(name, days_recent=30, max_items=200):\n",
    "    total, latest_dt, recent = fetch_news_features_for_company(name, days_recent, max_items)\n",
    "    time.sleep(0.5) \n",
    "    return total, latest_dt, recent\n",
    "news_df = pd.DataFrame({\n",
    "    'org_key': obj['org_key'],\n",
    "    'news_count': 0,\n",
    "    'latest_news_date': pd.NaT,\n",
    "    'recent_news_count': 0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58f7b421-89d2-402a-9d63-45f8ae14dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "master = (master\n",
    "    .merge(hq[['org_key','hq_city','hq_state','hq_country']], on='org_key', how='left')\n",
    "    .merge(num_offices, on='org_key', how='left')\n",
    "    .merge(funding, on='org_key', how='left')\n",
    "    .merge(lead_investors, on='org_key', how='left')\n",
    "    .merge(investors_count, on='org_key', how='left')\n",
    "    .merge(exit_flag[['org_key','exit_flag']], on='org_key', how='left')\n",
    "    .merge(ipo_at, on='org_key', how='left')\n",
    "    .merge(acquired_at, on='org_key', how='left')\n",
    "    .merge(milestones_count, on='org_key', how='left')\n",
    "    .merge(milestone_latest, on='org_key', how='left')\n",
    "    .merge(founders, on='org_key', how='left')\n",
    "    .merge(pd.DataFrame({'org_key': master['org_key'], 'num_employees': np.nan}), on='org_key', how='left')\n",
    "    .merge(pd.DataFrame({'org_key': master['org_key'], 'num_current_employees': np.nan}), on='org_key', how='left')\n",
    "    .merge(founder_education, on='org_key', how='left')\n",
    "    .merge(founder_universities, on='org_key', how='left')\n",
    "    .merge(num_investors, on='org_key', how='left', suffixes=('','_dup'))\n",
    "    .merge(news_df, on='org_key', how='left')\n",
    "    .merge(yc_subset, on='org_key', how='left')\n",
    "    .merge(vc_counts, on='org_key', how='left')\n",
    "    .merge(funds_feat, on='org_key', how='left')\n",
    ")\n",
    "if 'investors_count_dup' in master.columns:\n",
    "    master.drop(columns=['investors_count_dup'], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5220397-008f-458e-af7f-afb75c90e236",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_cols = [\n",
    "    \"name\",\"org_key\",\"category_code\",\"category_list\",\"company_stage\",\"status\",\"founded_at\",\"founded_year\",\n",
    "    \"hq_city\",\"hq_state\",\"hq_country\",\"num_offices\",\"company_age\",\n",
    "    \"funding_total_usd\",\"total_raised_usd\",\"num_rounds\",\"latest_round_type\",\"funding_per_year\",\"log_funding\",\n",
    "    \"first_funding_at\",\"last_funding_at\",\"funding_round_codes\",\"avg_round_size_usd\",\"lead_investors\",\"investors_count\",\n",
    "    \"exit_flag\",\"ipo_at\",\"acquired_at\",\"milestones_count\",\"milestone_latest\",\n",
    "    \"num_founders\",\"num_employees\",\"num_current_employees\",\"founder_education\",\"founder_universities\",\"num_investors\",\n",
    "    \"news_count\",\"latest_news_date\",\"recent_news_count\",\n",
    "    \"yc_batch\",\"yc_batch_year\",\"yc_status\",\"yc_top_company\",\"has_vc_backing\",\"vc_deal_count\",\"is_unicorn\",\n",
    "    \"num_funds\",\"funds_total_usd\",\"latest_fund_raised_at\"\n",
    "]\n",
    "for c in master_cols:\n",
    "    if c not in master.columns: master[c] = np.nan\n",
    "master = master[master_cols].drop_duplicates('org_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7759dd01-1f26-4b08-9ecd-5d79e7d6dc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'category_list' in master.columns:\n",
    "    master['category_list'] = normalize_list_like_series(master['category_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e5d7ed4-d647-48ce-98e2-578130784b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in master.select_dtypes(include='object').columns:\n",
    "    master[col] = (master[col].astype(str)\n",
    "                   .str.strip()\n",
    "                   .str.replace(r'\\s+', ' ', regex=True)\n",
    "                   .replace({'nan': np.nan, 'None': np.nan, '': np.nan}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b444fc8e-2af7-4169-aeaf-f0032e38b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\n",
    "    'company_age','num_offices','funding_total_usd','total_raised_usd','num_rounds','funding_per_year','log_funding',\n",
    "    'avg_round_size_usd','investors_count','num_investors','num_founders','num_employees','num_current_employees',\n",
    "    'milestones_count','exit_flag','news_count','recent_news_count','has_vc_backing','is_unicorn','yc_batch_year',\n",
    "    'vc_deal_count','yc_top_company','num_funds','funds_total_usd'\n",
    "]\n",
    "for col in num_cols:\n",
    "    master[col] = pd.to_numeric(master[col], errors='coerce')\n",
    "date_cols = ['founded_at','first_funding_at','last_funding_at','ipo_at','acquired_at','milestone_latest','latest_news_date','latest_fund_raised_at']\n",
    "for col in date_cols:\n",
    "    master[col] = pd.to_datetime(master[col], errors='coerce', utc=True)\n",
    "if 'company_age' in master.columns:\n",
    "    master['company_age'] = master['company_age'].clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98e0316c-8aa7-4c4e-a2e5-61db05314d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_fill = ['category_code','category_list','company_stage','status','hq_city','hq_state','hq_country',\n",
    "            'latest_round_type','lead_investors','founder_education','founder_universities','yc_batch','yc_status']\n",
    "for col in cat_fill:\n",
    "    master[col] = master[col].replace('nan', np.nan).fillna(\"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e502be9-4a44-4dd2-b98b-2bae46c61c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill0_cols = ['company_age','num_offices','funding_total_usd','total_raised_usd','num_rounds','funding_per_year','log_funding',\n",
    "              'avg_round_size_usd','investors_count','num_investors','num_founders','num_employees','num_current_employees',\n",
    "              'milestones_count','exit_flag','news_count','recent_news_count','has_vc_backing','is_unicorn','yc_batch_year',\n",
    "              'vc_deal_count','yc_top_company','num_funds','funds_total_usd']\n",
    "for col in fill0_cols:\n",
    "    master[col] = master[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e4e858d-03b7-43cc-b624-373d4682fef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ('exit_flag' not in master.columns) or master['exit_flag'].isnull().all():\n",
    "    statuses = master.get('status', pd.Series(dtype=str)).astype(str).str.lower()\n",
    "    master['exit_flag'] = (\n",
    "        statuses.isin(['exited','acquired','ipo'])\n",
    "        | master.get('ipo_at', pd.Series(index=master.index)).notnull()\n",
    "        | master.get('acquired_at', pd.Series(index=master.index)).notnull()\n",
    "    ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb10f317-066c-42c5-890a-234026ab7b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Wrote master datasets:\n",
      "- C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\master_startup_dataset.csv\n",
      "- C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\master_startup_dataset_clean.csv\n"
     ]
    }
   ],
   "source": [
    "p_master_raw = to_io(master, \"master_startup_dataset\")\n",
    "thresh = int(0.8 * master.shape[1])\n",
    "master = master.dropna(thresh=thresh).reset_index(drop=True)\n",
    "p_master_clean = to_io(master, \"master_startup_dataset_clean\")\n",
    "print(f\"[OK] Wrote master datasets:\\n- {p_master_raw}\\n- {p_master_clean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5346157d-c2a1-4512-815d-96469a01dea0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MASTER INFO]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 462648 entries, 0 to 462647\n",
      "Data columns (total 49 columns):\n",
      " #   Column                 Non-Null Count   Dtype              \n",
      "---  ------                 --------------   -----              \n",
      " 0   name                   462647 non-null  object             \n",
      " 1   org_key                462648 non-null  object             \n",
      " 2   category_code          462648 non-null  object             \n",
      " 3   category_list          462648 non-null  object             \n",
      " 4   company_stage          462648 non-null  object             \n",
      " 5   status                 462648 non-null  object             \n",
      " 6   founded_at             100441 non-null  datetime64[ns, UTC]\n",
      " 7   founded_year           100441 non-null  float64            \n",
      " 8   hq_city                462648 non-null  object             \n",
      " 9   hq_state               462648 non-null  object             \n",
      " 10  hq_country             462648 non-null  object             \n",
      " 11  num_offices            462648 non-null  float64            \n",
      " 12  company_age            462648 non-null  float64            \n",
      " 13  funding_total_usd      462648 non-null  float64            \n",
      " 14  total_raised_usd       462648 non-null  float64            \n",
      " 15  num_rounds             462648 non-null  float64            \n",
      " 16  latest_round_type      462648 non-null  object             \n",
      " 17  funding_per_year       462648 non-null  float64            \n",
      " 18  log_funding            462648 non-null  float64            \n",
      " 19  first_funding_at       31507 non-null   datetime64[ns, UTC]\n",
      " 20  last_funding_at        31507 non-null   datetime64[ns, UTC]\n",
      " 21  funding_round_codes    31707 non-null   object             \n",
      " 22  avg_round_size_usd     462648 non-null  float64            \n",
      " 23  lead_investors         462648 non-null  object             \n",
      " 24  investors_count        462648 non-null  float64            \n",
      " 25  exit_flag              462648 non-null  float64            \n",
      " 26  ipo_at                 653 non-null     datetime64[ns, UTC]\n",
      " 27  acquired_at            9366 non-null    datetime64[ns, UTC]\n",
      " 28  milestones_count       462648 non-null  float64            \n",
      " 29  milestone_latest       17009 non-null   datetime64[ns, UTC]\n",
      " 30  num_founders           462648 non-null  float64            \n",
      " 31  num_employees          462648 non-null  float64            \n",
      " 32  num_current_employees  462648 non-null  float64            \n",
      " 33  founder_education      462648 non-null  object             \n",
      " 34  founder_universities   462648 non-null  object             \n",
      " 35  num_investors          462648 non-null  float64            \n",
      " 36  news_count             462648 non-null  int64              \n",
      " 37  latest_news_date       0 non-null       datetime64[ns, UTC]\n",
      " 38  recent_news_count      462648 non-null  int64              \n",
      " 39  yc_batch               462648 non-null  object             \n",
      " 40  yc_batch_year          462648 non-null  float64            \n",
      " 41  yc_status              462648 non-null  object             \n",
      " 42  yc_top_company         462648 non-null  float64            \n",
      " 43  has_vc_backing         462648 non-null  float64            \n",
      " 44  vc_deal_count          462648 non-null  float64            \n",
      " 45  is_unicorn             462648 non-null  float64            \n",
      " 46  num_funds              462648 non-null  float64            \n",
      " 47  funds_total_usd        462648 non-null  float64            \n",
      " 48  latest_fund_raised_at  1005 non-null    datetime64[ns, UTC]\n",
      "dtypes: datetime64[ns, UTC](8), float64(23), int64(2), object(16)\n",
      "memory usage: 173.0+ MB\n",
      "None\n",
      "\n",
      "[MASTER DESCRIBE]\n",
      "                          count  unique        top    freq  \\\n",
      "name                     462647  445191  Bob Hebig      40   \n",
      "org_key                  462648  462648        c:1       1   \n",
      "category_code            462648      43    unknown  339462   \n",
      "category_list            462648       1    unknown  462648   \n",
      "company_stage            462648       5    unknown  444211   \n",
      "status                   462648       9  operating  443660   \n",
      "founded_at               100441     NaN        NaN     NaN   \n",
      "founded_year           100441.0     NaN        NaN     NaN   \n",
      "hq_city                  462648    3598    unknown  446269   \n",
      "hq_state                 462648      52    unknown  453483   \n",
      "hq_country               462648     127    unknown  445929   \n",
      "num_offices            462648.0     NaN        NaN     NaN   \n",
      "company_age            462648.0     NaN        NaN     NaN   \n",
      "funding_total_usd      462648.0     NaN        NaN     NaN   \n",
      "total_raised_usd       462648.0     NaN        NaN     NaN   \n",
      "num_rounds             462648.0     NaN        NaN     NaN   \n",
      "latest_round_type        462648      21    unknown  430941   \n",
      "funding_per_year       462648.0     NaN        NaN     NaN   \n",
      "log_funding            462648.0     NaN        NaN     NaN   \n",
      "first_funding_at          31507     NaN        NaN     NaN   \n",
      "last_funding_at           31507     NaN        NaN     NaN   \n",
      "funding_round_codes       31707     656       seed    7381   \n",
      "avg_round_size_usd     462648.0     NaN        NaN     NaN   \n",
      "lead_investors           462648      35    unknown  441163   \n",
      "investors_count        462648.0     NaN        NaN     NaN   \n",
      "exit_flag              462648.0     NaN        NaN     NaN   \n",
      "ipo_at                      653     NaN        NaN     NaN   \n",
      "acquired_at                9366     NaN        NaN     NaN   \n",
      "milestones_count       462648.0     NaN        NaN     NaN   \n",
      "milestone_latest          17009     NaN        NaN     NaN   \n",
      "num_founders           462648.0     NaN        NaN     NaN   \n",
      "num_employees          462648.0     NaN        NaN     NaN   \n",
      "num_current_employees  462648.0     NaN        NaN     NaN   \n",
      "founder_education        462648    4043    unknown  441949   \n",
      "founder_universities     462648   11940    unknown  439890   \n",
      "num_investors          462648.0     NaN        NaN     NaN   \n",
      "news_count             462648.0     NaN        NaN     NaN   \n",
      "latest_news_date              0     NaN        NaN     NaN   \n",
      "recent_news_count      462648.0     NaN        NaN     NaN   \n",
      "yc_batch                 462648       1    unknown  462648   \n",
      "yc_batch_year          462648.0     NaN        NaN     NaN   \n",
      "yc_status                462648       1    unknown  462648   \n",
      "yc_top_company         462648.0     NaN        NaN     NaN   \n",
      "has_vc_backing         462648.0     NaN        NaN     NaN   \n",
      "vc_deal_count          462648.0     NaN        NaN     NaN   \n",
      "is_unicorn             462648.0     NaN        NaN     NaN   \n",
      "num_funds              462648.0     NaN        NaN     NaN   \n",
      "funds_total_usd        462648.0     NaN        NaN     NaN   \n",
      "latest_fund_raised_at      1005     NaN        NaN     NaN   \n",
      "\n",
      "                                                      mean  \\\n",
      "name                                                   NaN   \n",
      "org_key                                                NaN   \n",
      "category_code                                          NaN   \n",
      "category_list                                          NaN   \n",
      "company_stage                                          NaN   \n",
      "status                                                 NaN   \n",
      "founded_at             2005-09-18 13:45:03.163051264+00:00   \n",
      "founded_year                                   2005.463177   \n",
      "hq_city                                                NaN   \n",
      "hq_state                                               NaN   \n",
      "hq_country                                             NaN   \n",
      "num_offices                                       0.243637   \n",
      "company_age                                       4.241449   \n",
      "funding_total_usd                            892677.997564   \n",
      "total_raised_usd                             892677.997577   \n",
      "num_rounds                                         0.11375   \n",
      "latest_round_type                                      NaN   \n",
      "funding_per_year                             550114.007106   \n",
      "log_funding                                       0.382161   \n",
      "first_funding_at       2010-06-11 11:43:03.997206784+00:00   \n",
      "last_funding_at        2011-04-12 14:50:51.893229824+00:00   \n",
      "funding_round_codes                                    NaN   \n",
      "avg_round_size_usd                           468612.879793   \n",
      "lead_investors                                         NaN   \n",
      "investors_count                                   0.138211   \n",
      "exit_flag                                         0.022756   \n",
      "ipo_at                 2004-07-22 11:56:41.531393792+00:00   \n",
      "acquired_at            2009-12-28 12:07:32.017937408+00:00   \n",
      "milestones_count                                  0.084531   \n",
      "milestone_latest       2011-09-13 16:40:31.465694464+00:00   \n",
      "num_founders                                      0.150713   \n",
      "num_employees                                          0.0   \n",
      "num_current_employees                                  0.0   \n",
      "founder_education                                      NaN   \n",
      "founder_universities                                   NaN   \n",
      "num_investors                                          0.0   \n",
      "news_count                                             0.0   \n",
      "latest_news_date                                       NaT   \n",
      "recent_news_count                                      0.0   \n",
      "yc_batch                                               NaN   \n",
      "yc_batch_year                                          0.0   \n",
      "yc_status                                              NaN   \n",
      "yc_top_company                                         0.0   \n",
      "has_vc_backing                                         0.0   \n",
      "vc_deal_count                                          0.0   \n",
      "is_unicorn                                             0.0   \n",
      "num_funds                                         0.003329   \n",
      "funds_total_usd                             1341735.849274   \n",
      "latest_fund_raised_at  2012-08-02 03:38:47.957213952+00:00   \n",
      "\n",
      "                                             min                        25%  \\\n",
      "name                                         NaN                        NaN   \n",
      "org_key                                      NaN                        NaN   \n",
      "category_code                                NaN                        NaN   \n",
      "category_list                                NaN                        NaN   \n",
      "company_stage                                NaN                        NaN   \n",
      "status                                       NaN                        NaN   \n",
      "founded_at             1901-01-01 00:00:00+00:00  2004-01-01 00:00:00+00:00   \n",
      "founded_year                              1901.0                     2004.0   \n",
      "hq_city                                      NaN                        NaN   \n",
      "hq_state                                     NaN                        NaN   \n",
      "hq_country                                   NaN                        NaN   \n",
      "num_offices                                  0.0                        0.0   \n",
      "company_age                                  0.0                        0.0   \n",
      "funding_total_usd                            0.0                        0.0   \n",
      "total_raised_usd                             0.0                        0.0   \n",
      "num_rounds                                   0.0                        0.0   \n",
      "latest_round_type                            NaN                        NaN   \n",
      "funding_per_year                             0.0                        0.0   \n",
      "log_funding                                  0.0                        0.0   \n",
      "first_funding_at       1960-01-01 00:00:00+00:00  2008-09-01 00:00:00+00:00   \n",
      "last_funding_at        1960-01-01 00:00:00+00:00  2010-01-10 00:00:00+00:00   \n",
      "funding_round_codes                          NaN                        NaN   \n",
      "avg_round_size_usd                           0.0                        0.0   \n",
      "lead_investors                               NaN                        NaN   \n",
      "investors_count                              0.0                        0.0   \n",
      "exit_flag                                    0.0                        0.0   \n",
      "ipo_at                 1969-06-09 00:00:00+00:00  1999-08-01 00:00:00+00:00   \n",
      "acquired_at            1967-04-07 00:00:00+00:00  2008-10-08 06:00:00+00:00   \n",
      "milestones_count                             0.0                        0.0   \n",
      "milestone_latest       1963-01-01 00:00:00+00:00  2010-11-01 00:00:00+00:00   \n",
      "num_founders                                 0.0                        0.0   \n",
      "num_employees                                0.0                        0.0   \n",
      "num_current_employees                        0.0                        0.0   \n",
      "founder_education                            NaN                        NaN   \n",
      "founder_universities                         NaN                        NaN   \n",
      "num_investors                                0.0                        0.0   \n",
      "news_count                                   0.0                        0.0   \n",
      "latest_news_date                             NaT                        NaT   \n",
      "recent_news_count                            0.0                        0.0   \n",
      "yc_batch                                     NaN                        NaN   \n",
      "yc_batch_year                                0.0                        0.0   \n",
      "yc_status                                    NaN                        NaN   \n",
      "yc_top_company                               0.0                        0.0   \n",
      "has_vc_backing                               0.0                        0.0   \n",
      "vc_deal_count                                0.0                        0.0   \n",
      "is_unicorn                                   0.0                        0.0   \n",
      "num_funds                                    0.0                        0.0   \n",
      "funds_total_usd                              0.0                        0.0   \n",
      "latest_fund_raised_at  2008-12-17 03:07:16+00:00  2011-11-01 16:04:27+00:00   \n",
      "\n",
      "                                             50%                        75%  \\\n",
      "name                                         NaN                        NaN   \n",
      "org_key                                      NaN                        NaN   \n",
      "category_code                                NaN                        NaN   \n",
      "category_list                                NaN                        NaN   \n",
      "company_stage                                NaN                        NaN   \n",
      "status                                       NaN                        NaN   \n",
      "founded_at             2008-12-01 00:00:00+00:00  2011-01-14 00:00:00+00:00   \n",
      "founded_year                              2008.0                     2011.0   \n",
      "hq_city                                      NaN                        NaN   \n",
      "hq_state                                     NaN                        NaN   \n",
      "hq_country                                   NaN                        NaN   \n",
      "num_offices                                  0.0                        0.0   \n",
      "company_age                                  0.0                        0.0   \n",
      "funding_total_usd                            0.0                        0.0   \n",
      "total_raised_usd                             0.0                        0.0   \n",
      "num_rounds                                   0.0                        0.0   \n",
      "latest_round_type                            NaN                        NaN   \n",
      "funding_per_year                             0.0                        0.0   \n",
      "log_funding                                  0.0                        0.0   \n",
      "first_funding_at       2011-02-22 00:00:00+00:00  2012-08-01 00:00:00+00:00   \n",
      "last_funding_at        2012-01-01 00:00:00+00:00  2013-03-20 00:00:00+00:00   \n",
      "funding_round_codes                          NaN                        NaN   \n",
      "avg_round_size_usd                           0.0                        0.0   \n",
      "lead_investors                               NaN                        NaN   \n",
      "investors_count                              0.0                        0.0   \n",
      "exit_flag                                    0.0                        0.0   \n",
      "ipo_at                 2008-01-17 00:00:00+00:00  2011-12-19 00:00:00+00:00   \n",
      "acquired_at            2010-08-31 00:00:00+00:00  2012-03-09 18:00:00+00:00   \n",
      "milestones_count                             0.0                        0.0   \n",
      "milestone_latest       2012-01-09 00:00:00+00:00  2013-04-01 00:00:00+00:00   \n",
      "num_founders                                 0.0                        0.0   \n",
      "num_employees                                0.0                        0.0   \n",
      "num_current_employees                        0.0                        0.0   \n",
      "founder_education                            NaN                        NaN   \n",
      "founder_universities                         NaN                        NaN   \n",
      "num_investors                                0.0                        0.0   \n",
      "news_count                                   0.0                        0.0   \n",
      "latest_news_date                             NaT                        NaT   \n",
      "recent_news_count                            0.0                        0.0   \n",
      "yc_batch                                     NaN                        NaN   \n",
      "yc_batch_year                                0.0                        0.0   \n",
      "yc_status                                    NaN                        NaN   \n",
      "yc_top_company                               0.0                        0.0   \n",
      "has_vc_backing                               0.0                        0.0   \n",
      "vc_deal_count                                0.0                        0.0   \n",
      "is_unicorn                                   0.0                        0.0   \n",
      "num_funds                                    0.0                        0.0   \n",
      "funds_total_usd                              0.0                        0.0   \n",
      "latest_fund_raised_at  2013-02-08 23:09:32+00:00  2013-08-16 09:19:02+00:00   \n",
      "\n",
      "                                             max               std  \n",
      "name                                         NaN               NaN  \n",
      "org_key                                      NaN               NaN  \n",
      "category_code                                NaN               NaN  \n",
      "category_list                                NaN               NaN  \n",
      "company_stage                                NaN               NaN  \n",
      "status                                       NaN               NaN  \n",
      "founded_at             2014-10-01 00:00:00+00:00               NaN  \n",
      "founded_year                              2014.0         10.215455  \n",
      "hq_city                                      NaN               NaN  \n",
      "hq_state                                     NaN               NaN  \n",
      "hq_country                                   NaN               NaN  \n",
      "num_offices                                 81.0          0.605092  \n",
      "company_age                                124.0          9.355753  \n",
      "funding_total_usd                   5700000000.0   17001251.954469  \n",
      "total_raised_usd                    5700000000.0   17001251.954469  \n",
      "num_rounds                                  15.0          0.524231  \n",
      "latest_round_type                            NaN               NaN  \n",
      "funding_per_year                   10774875000.0    32380838.74835  \n",
      "log_funding                             9.755875          1.527469  \n",
      "first_funding_at       2013-12-12 00:00:00+00:00               NaN  \n",
      "last_funding_at        2013-12-12 00:00:00+00:00               NaN  \n",
      "funding_round_codes                          NaN               NaN  \n",
      "avg_round_size_usd                  2600000000.0    9477692.520093  \n",
      "lead_investors                               NaN               NaN  \n",
      "investors_count                             49.0          0.891338  \n",
      "exit_flag                                    1.0          0.149125  \n",
      "ipo_at                 2013-12-11 00:00:00+00:00               NaN  \n",
      "acquired_at            2013-12-12 00:00:00+00:00               NaN  \n",
      "milestones_count                            75.0          0.776645  \n",
      "milestone_latest       2014-12-31 00:00:00+00:00               NaN  \n",
      "num_founders                                26.0          0.521458  \n",
      "num_employees                                0.0               0.0  \n",
      "num_current_employees                        0.0               0.0  \n",
      "founder_education                            NaN               NaN  \n",
      "founder_universities                         NaN               NaN  \n",
      "num_investors                                0.0               0.0  \n",
      "news_count                                   0.0               0.0  \n",
      "latest_news_date                             NaT               NaN  \n",
      "recent_news_count                            0.0               0.0  \n",
      "yc_batch                                     NaN               NaN  \n",
      "yc_batch_year                                0.0               0.0  \n",
      "yc_status                                    NaN               NaN  \n",
      "yc_top_company                               0.0               0.0  \n",
      "has_vc_backing                               0.0               0.0  \n",
      "vc_deal_count                                0.0               0.0  \n",
      "is_unicorn                                   0.0               0.0  \n",
      "num_funds                                   10.0          0.088784  \n",
      "funds_total_usd                    89000000000.0  152910332.525476  \n",
      "latest_fund_raised_at  2013-12-12 09:42:12+00:00               NaN  \n",
      "\n",
      "[MASTER NULLS TOP-20]\n",
      "latest_news_date         462648\n",
      "ipo_at                   461995\n",
      "latest_fund_raised_at    461643\n",
      "acquired_at              453282\n",
      "milestone_latest         445639\n",
      "first_funding_at         431141\n",
      "last_funding_at          431141\n",
      "funding_round_codes      430941\n",
      "founded_at               362207\n",
      "founded_year             362207\n",
      "name                          1\n",
      "num_offices                   0\n",
      "yc_batch                      0\n",
      "num_current_employees         0\n",
      "founder_education             0\n",
      "founder_universities          0\n",
      "num_investors                 0\n",
      "news_count                    0\n",
      "category_code                 0\n",
      "recent_news_count             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[MASTER INFO]\")\n",
    "print(master.info())\n",
    "print(\"\\n[MASTER DESCRIBE]\")\n",
    "print(master.describe(include='all').T)\n",
    "print(\"\\n[MASTER NULLS TOP-20]\")\n",
    "print(master.isnull().sum().sort_values(ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da16efa3-aafa-4f5d-af93-bd08c27fbb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "faker = Faker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b6bc4b93-b18c-417d-863b-29c8e4c5961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bad_date():\n",
    "    if np.random.rand() < 0.2: return None\n",
    "    dtt = faker.date_between(start_date='-25y', end_date='today')\n",
    "    r = np.random.rand()\n",
    "    if r < 0.2: return dtt.strftime(\"%Y/%m/%d\")\n",
    "    if r < 0.4: return dtt.strftime(\"%m-%d-%Y\")\n",
    "    if r < 0.6: return str(dtt.year)\n",
    "    return dtt\n",
    "def rand_bad_str():\n",
    "    s = faker.company()\n",
    "    s = ''.join([c.upper() if np.random.rand() < 0.2 else c for c in s])\n",
    "    if np.random.rand() < 0.15: s += ' ' * random.randint(1,4)\n",
    "    if np.random.rand() < 0.1: s += str(random.randint(100,999))\n",
    "    if np.random.rand() < 0.1: s = s.replace(' ', '  ')\n",
    "    if np.random.rand() < 0.05: s = s + \"NA\"\n",
    "    return s\n",
    "def rand_bad_num(minv, maxv):\n",
    "    r = np.random.rand()\n",
    "    if r < 0.1: return None\n",
    "    if r < 0.2: return random.choice([\"unknown\", \"NaN\", \"zero\", \" \"])\n",
    "    if r < 0.3: return random.choice([\"$\", \",\", \".\", \"n/a\"])\n",
    "    if r < 0.5: return str(np.random.randint(minv, maxv))\n",
    "    return np.random.randint(minv, maxv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed41b5f9-a5cd-4d48-b872-39d5827769fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_category():\n",
    "    return random.choice([\"fintech\", \"AI\", \"health\", \"gaming\", \"e-commerce\", None, \"    ai\", \"cloud  \", \"food\", \"medtech\", \"hrtech\", \"AI \"])\n",
    "def rand_stage():\n",
    "    return random.choice([\"Seed\", \"Series A\", \"Series B\", \"Pre-Seed\", \"Late\", None, \"Series Z\", \"Bridge\", \"unknown\"])\n",
    "def rand_status():\n",
    "    return random.choice([\"operating\", \"dead\", \"exited\", \"unknown\", None, \"in limbo\", \"Acquired\", \"IPO\"])\n",
    "def rand_yc():\n",
    "    return random.choice([None, \"W18\", \"S20\", \"W22\", \"S16\", \"S21\", \"unknown\"])\n",
    "def rand_bool():\n",
    "    return random.choice([1, 0, None, \"True\", \"False\", \"yes\", \"no\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "82ed23c0-4b71-4ac5-b130-59026d42f6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_cols = [\n",
    "    \"name\",\"org_key\",\"category_code\",\"category_list\",\"company_stage\",\"status\",\"founded_at\",\"founded_year\",\n",
    "    \"hq_city\",\"hq_state\",\"hq_country\",\"num_offices\",\"company_age\",\n",
    "    \"funding_total_usd\",\"total_raised_usd\",\"num_rounds\",\"latest_round_type\",\"funding_per_year\",\"log_funding\",\n",
    "    \"first_funding_at\",\"last_funding_at\",\"funding_round_codes\",\"avg_round_size_usd\",\"lead_investors\",\"investors_count\",\n",
    "    \"exit_flag\",\"ipo_at\",\"acquired_at\",\"milestones_count\",\"milestone_latest\",\n",
    "    \"num_founders\",\"num_employees\",\"num_current_employees\",\"founder_education\",\"founder_universities\",\"num_investors\",\n",
    "    \"news_count\",\"latest_news_date\",\"recent_news_count\",\n",
    "    \"yc_batch\",\"yc_batch_year\",\"yc_status\",\"yc_top_company\",\"has_vc_backing\",\"vc_deal_count\",\"is_unicorn\"\n",
    "]\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92e960e0-1c2f-4a7f-9d82-5fb069364285",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(SYNTHETIC_N):\n",
    "    rows.append([\n",
    "        rand_bad_str(), f\"org_{i:05d}\",\n",
    "        rand_category(), rand_category(), rand_stage(), rand_status(),\n",
    "        rand_bad_date(), rand_bad_num(1980,2024),\n",
    "        rand_bad_str(), rand_bad_str(), rand_bad_str(),\n",
    "        rand_bad_num(0,10), rand_bad_num(0,30),\n",
    "        rand_bad_num(0,1_000_000_000), rand_bad_num(0,1_000_000_000),\n",
    "        rand_bad_num(0,50), rand_stage(), rand_bad_num(0,50_000_000),\n",
    "        rand_bad_num(0,12), rand_bad_date(), rand_bad_date(),\n",
    "        \",\".join([rand_bad_str() for _ in range(random.randint(1,3))]),\n",
    "        rand_bad_num(0,100_000_000), rand_bad_str(), rand_bad_num(0,30),\n",
    "        rand_bool(), rand_bad_date(), rand_bad_date(),\n",
    "        rand_bad_num(0,10), rand_bad_date(),\n",
    "        rand_bad_num(1,6), rand_bad_num(0,1000), rand_bad_num(0,1000),\n",
    "        rand_bad_str(), rand_bad_str(), rand_bad_num(0,100),\n",
    "        rand_bad_num(0,20), rand_bad_date(), rand_bad_num(0,10),\n",
    "        rand_yc(), rand_bad_num(2010,2023), rand_status(),\n",
    "        rand_bool(), rand_bool(), rand_bad_num(0,40), rand_bool()\n",
    "    ])\n",
    "synthetic_master = pd.DataFrame(rows, columns=syn_cols)\n",
    "p_syn_raw = to_io(synthetic_master, \"synthetic_master_startup_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "620d3738-a2b0-4e65-aa4c-5f8f4b5bf73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_master = from_io(\"synthetic_master_startup_dataset\").drop_duplicates('org_key')\n",
    "for col in synthetic_master.select_dtypes(include='object').columns:\n",
    "    synthetic_master[col] = (synthetic_master[col].astype(str)\n",
    "                             .str.strip()\n",
    "                             .str.replace(r'\\s+',' ',regex=True)\n",
    "                             .replace({\"nan\": np.nan, \"None\": np.nan, \"NAN\": np.nan, \"\": np.nan}))\n",
    "cat_fill_syn = ['category_code','category_list','company_stage','status','hq_city','hq_state','hq_country',\n",
    "                'latest_round_type','lead_investors','founder_education','founder_universities','yc_batch','yc_status']\n",
    "for col in cat_fill_syn:\n",
    "    if col in synthetic_master.columns:\n",
    "        synthetic_master[col] = (synthetic_master[col].astype(str).str.lower()\n",
    "                                 .replace(['none','unknown','n/a','nan'],np.nan).fillna(\"unknown\"))\n",
    "num_cols_syn = ['company_age','num_offices','funding_total_usd','total_raised_usd','num_rounds','funding_per_year','log_funding',\n",
    "                'avg_round_size_usd','investors_count','num_investors','num_founders','num_employees','num_current_employees',\n",
    "                'milestones_count','exit_flag','news_count','recent_news_count','has_vc_backing','is_unicorn','yc_batch_year',\n",
    "                'vc_deal_count','yc_top_company']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "81a02622-448e-46df-9787-16bdb27189bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Wrote synthetic datasets:\n",
      "- C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\synthetic_master_startup_dataset.csv\n",
      "- C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\AIVA\\synthetic_master_startup_dataset_clean.csv\n"
     ]
    }
   ],
   "source": [
    "def parse_numeric(val):\n",
    "    try:\n",
    "        if pd.isna(val) or str(val).strip().lower() in ['', 'nan','none','n/a','unknown','-','?']: return np.nan\n",
    "        s = str(val).lower().replace(\",\",\"\").replace(\"$\",\"\").replace(\" \",\"\")\n",
    "        if s.endswith(\"m\"): return float(s[:-1]) * 1e6\n",
    "        if s.endswith(\"k\"): return float(s[:-1]) * 1e3\n",
    "        return float(s)\n",
    "    except: return np.nan\n",
    "for col in num_cols_syn:\n",
    "    if col in synthetic_master.columns:\n",
    "        synthetic_master[col] = synthetic_master[col].apply(parse_numeric)\n",
    "        if col != 'company_age':\n",
    "            synthetic_master[col] = synthetic_master[col].abs()\n",
    "if 'company_age' in synthetic_master.columns:\n",
    "    synthetic_master['company_age'] = synthetic_master['company_age'].clip(lower=0)\n",
    "date_cols_syn = ['founded_at','first_funding_at','last_funding_at','ipo_at','acquired_at','milestone_latest','latest_news_date']\n",
    "for col in date_cols_syn:\n",
    "    if col in synthetic_master.columns:\n",
    "        synthetic_master[col] = pd.to_datetime(synthetic_master[col], errors='coerce', utc=True)\n",
    "for col in cat_fill_syn:\n",
    "    if col in synthetic_master.columns:\n",
    "        synthetic_master[col] = synthetic_master[col].fillna(\"unknown\")\n",
    "for col in num_cols_syn:\n",
    "    if col in synthetic_master.columns:\n",
    "        synthetic_master[col] = synthetic_master[col].fillna(0)\n",
    "synthetic_master = synthetic_master.dropna(thresh=int(0.8*synthetic_master.shape[1])).reset_index(drop=True)\n",
    "p_syn_clean = to_io(synthetic_master, \"synthetic_master_startup_dataset_clean\")\n",
    "print(f\"[OK] Wrote synthetic datasets:\\n- {p_syn_raw}\\n- {p_syn_clean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ca20852-9ac5-4493-a0ed-77e0b6971b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-start RF on synthetic:  20%|                                                                        | 2/10 [00:00<00:01,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/10 | size=5000 | acc=0.999\n",
      "Batch 2/10 | size=5000 | acc=0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-start RF on synthetic:  40%|                                                      | 4/10 [00:00<00:01,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3/10 | size=5000 | acc=0.858\n",
      "Batch 4/10 | size=5000 | acc=0.849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-start RF on synthetic:  50%|                                             | 5/10 [00:01<00:00,  5.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5/10 | size=5000 | acc=0.854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-start RF on synthetic:  70%|                           | 7/10 [00:01<00:00,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6/10 | size=5000 | acc=0.857\n",
      "Batch 7/10 | size=5000 | acc=0.852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-start RF on synthetic:  90%|         | 9/10 [00:01<00:00,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8/10 | size=5000 | acc=0.857\n",
      "Batch 9/10 | size=5000 | acc=0.862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-start RF on synthetic: 100%|| 10/10 [00:01<00:00,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10/10 | size=5000 | acc=0.870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "synthetic_ws = from_io(\"synthetic_master_startup_dataset_clean\")\n",
    "if \"exit_flag\" not in synthetic_ws.columns or synthetic_ws[\"exit_flag\"].isnull().all():\n",
    "    np.random.seed(RAND_SEED)\n",
    "    synthetic_ws[\"exit_flag\"] = np.random.randint(0, 2, size=len(synthetic_ws))\n",
    "target = \"exit_flag\"\n",
    "features_ws = [c for c in synthetic_ws.columns if c not in [target, 'org_key', 'name']]\n",
    "cat_cols_ws = synthetic_ws[features_ws].select_dtypes(include='object').columns\n",
    "for col in cat_cols_ws:\n",
    "    synthetic_ws[col] = synthetic_ws[col].astype(str)\n",
    "    le = LabelEncoder()\n",
    "    synthetic_ws[col] = le.fit_transform(synthetic_ws[col])\n",
    "for col in features_ws:\n",
    "    synthetic_ws[col] = pd.to_numeric(synthetic_ws[col], errors='coerce').fillna(-1)\n",
    "synthetic_ws[target] = pd.to_numeric(synthetic_ws[target], errors='coerce').fillna(0).astype(int)\n",
    "n_batches = int(np.ceil(len(synthetic_ws) / RF_WARM_BATCH))\n",
    "model_ws = RandomForestClassifier(n_estimators=50, random_state=RAND_SEED, n_jobs=-1, warm_start=True)\n",
    "for b in tqdm(range(n_batches), desc=\"Warm-start RF on synthetic\"):\n",
    "    s = b * RF_WARM_BATCH\n",
    "    e = min((b+1) * RF_WARM_BATCH, len(synthetic_ws))\n",
    "    Xb = synthetic_ws.iloc[s:e][features_ws]\n",
    "    yb = synthetic_ws.iloc[s:e][target]\n",
    "    if b == 0:\n",
    "        model_ws.fit(Xb, yb)\n",
    "    else:\n",
    "        model_ws.n_estimators += 10\n",
    "        model_ws.fit(Xb, yb)\n",
    "    print(f\"Batch {b+1}/{n_batches} | size={len(Xb)} | acc={accuracy_score(yb, model_ws.predict(Xb)):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2a508261-5768-48c8-8358-09cbb6d85e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synthetic warm-start] accuracy: 0.8582\n",
      "[Synthetic] after hard-example retrain acc: 0.85824\n"
     ]
    }
   ],
   "source": [
    "X_full = synthetic_ws[features_ws]; y_full = synthetic_ws[target]\n",
    "pred_full = model_ws.predict(X_full)\n",
    "print(f\"[Synthetic warm-start] accuracy: {accuracy_score(y_full, pred_full):.4f}\")\n",
    "errs = synthetic_ws[y_full != pred_full].copy()\n",
    "hard = resample(errs, replace=True, n_samples=max(len(errs)*2, 1), random_state=RAND_SEED)\n",
    "train_aug = pd.concat([synthetic_ws, hard], ignore_index=True)\n",
    "model_ws.fit(train_aug[features_ws], train_aug[target])\n",
    "print(\"[Synthetic] after hard-example retrain acc:\", accuracy_score(y_full, model_ws.predict(X_full)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa256cd4-4b3a-4b43-b80f-9dd3e79ea725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=140, n_jobs=-1, random_state=42,\n",
       "                       warm_start=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(n_estimators=140, n_jobs=-1, random_state=42,\n",
       "                       warm_start=True)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=140, n_jobs=-1, random_state=42,\n",
       "                       warm_start=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_ws = [c for c in features_ws if np.issubdtype(synthetic_ws[c].dtype, np.number)]\n",
    "for c in num_ws:\n",
    "    if c in errs:\n",
    "        errs[c] = pd.to_numeric(errs[c], errors='coerce').fillna(0) + np.random.normal(0, 0.1*(errs[c].std()+1), size=len(errs))\n",
    "train_mix = pd.concat([synthetic_ws, errs], ignore_index=True)\n",
    "model_ws.fit(train_mix[features_ws], train_mix[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2bf5690a-1ba4-4882-be8e-699ba9dff5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "master = from_io(\"master_startup_dataset_clean\")\n",
    "synthetic = from_io(\"synthetic_master_startup_dataset_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a86e9f32-d13f-4294-b5ac-7aee1f3082fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'exit_flag' not in master.columns or master['exit_flag'].isnull().all():\n",
    "    master['exit_flag'] = (\n",
    "        master['status'].astype(str).str.lower().isin(['exited','acquired','ipo'])\n",
    "        | master['ipo_at'].notnull()\n",
    "        | master['acquired_at'].notnull()\n",
    "    ).astype(int)\n",
    "target = \"exit_flag\"\n",
    "features_train = [c for c in synthetic.columns if c not in [target, 'org_key', 'name']]\n",
    "for c in features_train:\n",
    "    if c not in master.columns:\n",
    "        master[c] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aab00c6d-c2b7-4b7b-9717-3f05ceb12c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = master[features_train].select_dtypes(include='object').columns.tolist()\n",
    "encoders = {}\n",
    "ohe = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "656ec31d-1053-4c9c-881b-28a922dc7500",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_OHE and len(cat_cols) > 0:\n",
    "    all_cats_df = pd.concat([synthetic[cat_cols].astype(str), master[cat_cols].astype(str)], axis=0)\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    ohe.fit(all_cats_df.values)\n",
    "else:\n",
    "    for col in cat_cols:\n",
    "        vocab = pd.concat([synthetic[col].astype(str), master[col].astype(str)], axis=0).drop_duplicates().fillna(\"unknown\")\n",
    "        le = LabelEncoder().fit(vocab)\n",
    "        synthetic[col] = le.transform(synthetic[col].astype(str).fillna(\"unknown\"))\n",
    "        master[col]    = le.transform(master[col].astype(str).fillna(\"unknown\"))\n",
    "        encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "504331e0-e56a-4aee-a5ff-17941c84b52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_ENCODERS:\n",
    "    with open(os.path.join(OUT_ROOT, \"label_encoders.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(encoders, f)\n",
    "    if ohe is not None:\n",
    "        with open(os.path.join(OUT_ROOT, \"ohe_encoder.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(ohe, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e85cc14-0eac-461d-bcc2-d6cb25ef0991",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in (synthetic, master):\n",
    "    for col in features_train:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "    df[target] = pd.to_numeric(df[target], errors='coerce').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ed450a5b-31b6-487a-810f-964c7f397d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] PED model trained on synthetic (balanced)\n"
     ]
    }
   ],
   "source": [
    "def build_X(df):\n",
    "    if USE_OHE and ohe is not None and len(cat_cols) > 0:\n",
    "        X_num = df[[c for c in features_train if c not in cat_cols]].values\n",
    "        X_cat = ohe.transform(df[cat_cols].astype(str).values)\n",
    "        return np.hstack([X_num, X_cat])\n",
    "    else:\n",
    "        return df[features_train].values\n",
    "X_syn = build_X(synthetic); y_syn = synthetic[target].values\n",
    "X_mas = build_X(master);    y_mas = master[target].values\n",
    "model = RandomForestClassifier(n_estimators=300, class_weight='balanced', random_state=RAND_SEED, n_jobs=-1)\n",
    "model.fit(X_syn, y_syn)\n",
    "print(\"[OK] PED model trained on synthetic (balanced)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e7517a77-cb3c-4159-aee3-6e02b4498f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predict master: 100%|| 47/47 [00:05<00:00,  8.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Master Eval (baseline) ===\n",
      "Accuracy: 0.9772440386643841\n",
      "Confusion:\n",
      " [[452120      0]\n",
      " [ 10528      0]]\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9772    1.0000    0.9885    452120\n",
      "           1     0.0000    0.0000    0.0000     10528\n",
      "\n",
      "    accuracy                         0.9772    462648\n",
      "   macro avg     0.4886    0.5000    0.4942    462648\n",
      "weighted avg     0.9550    0.9772    0.9660    462648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH = 10000\n",
    "all_pred, all_prob = [], []\n",
    "for i in tqdm(range(0, len(master), BATCH), desc=\"Predict master\"):\n",
    "    Xi = X_mas[i:i+BATCH]\n",
    "    pi = model.predict_proba(Xi)[:,1]\n",
    "    yi = (pi > PRED_THRESH).astype(int)\n",
    "    all_prob.extend(pi); all_pred.extend(yi)\n",
    "y_prob = np.array(all_prob); y_pred = np.array(all_pred)\n",
    "print(\"\\n=== Master Eval (baseline) ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_mas, y_pred))\n",
    "print(\"Confusion:\\n\", confusion_matrix(y_mas, y_pred))\n",
    "print(\"Report:\\n\", classification_report(y_mas, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f58ee5ed-e73f-4849-a671-85367afee808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.369018895521793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\aniru\\\\OneDrive\\\\Desktop\\\\ML tutorial\\\\AIVA\\\\master_PED_predictions_full.csv'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try: print(\"ROC-AUC:\", roc_auc_score(y_mas, y_prob))\n",
    "except: pass\n",
    "master['PED_pred']  = y_pred\n",
    "master['PED_proba'] = y_prob\n",
    "to_io(master, \"master_PED_predictions_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8499d340-9501-48b6-bf36-fd88768a2916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 15 features:\n",
      " founder_universities     0.042903\n",
      "hq_state                 0.042822\n",
      "hq_city                  0.042346\n",
      "lead_investors           0.042107\n",
      "hq_country               0.042040\n",
      "funding_round_codes      0.041920\n",
      "founder_education        0.041814\n",
      "avg_round_size_usd       0.032212\n",
      "total_raised_usd         0.031816\n",
      "funding_per_year         0.031696\n",
      "funding_total_usd        0.031579\n",
      "num_current_employees    0.031506\n",
      "num_employees            0.031361\n",
      "num_investors            0.028645\n",
      "founded_year             0.027068\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "if not USE_OHE:\n",
    "    try:\n",
    "        imp = pd.Series(model.feature_importances_, index=features_train).sort_values(ascending=False)\n",
    "        print(\"\\nTop 15 features:\\n\", imp.head(15))\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3b99aa1b-d143-43dd-b795-19f5fddd42bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard errors: 10528 / 462648 (2.28%)\n"
     ]
    }
   ],
   "source": [
    "mask_err = master[target] != master['PED_pred']\n",
    "errs = master[mask_err].copy()\n",
    "print(f\"Hard errors: {len(errs)} / {len(master)} ({len(errs)/max(1,len(master)):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ccb51364-9200-41c1-9c54-42cabbec413b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== After bootcamp (refined) ===\n",
      "Accuracy: 0.9984221265411285\n",
      "Confusion:\n",
      " [[451391    729]\n",
      " [     1  10527]]\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9984    0.9992    452120\n",
      "           1     0.9352    0.9999    0.9665     10528\n",
      "\n",
      "    accuracy                         0.9984    462648\n",
      "   macro avg     0.9676    0.9991    0.9828    462648\n",
      "weighted avg     0.9985    0.9984    0.9984    462648\n",
      "\n",
      "ROC-AUC: 0.9999993805567328\n"
     ]
    }
   ],
   "source": [
    "if len(errs) > 0:\n",
    "    corr = master[~mask_err]\n",
    "    negs = corr[corr[target]==0]\n",
    "    neg_sample = resample(negs, replace=False, n_samples=min(len(errs), len(negs)), random_state=RAND_SEED)\n",
    "    boot = pd.concat([errs, neg_sample], ignore_index=True)\n",
    "    for col in features_train:\n",
    "        if np.issubdtype(master[col].dtype, np.number):\n",
    "            stdv = master[col].std()\n",
    "            if pd.notna(stdv) and stdv > 0:\n",
    "                boot[col] = pd.to_numeric(boot[col], errors='coerce').fillna(0) + np.random.normal(0, 0.05*(stdv+1e-6), size=len(boot))\n",
    "\n",
    "    X_boot = build_X(boot); y_boot = boot[target].values\n",
    "    model.fit(X_boot, y_boot)\n",
    "    master['PED_pred_refined']  = model.predict(build_X(master))\n",
    "    try: master['PED_proba_refined'] = model.predict_proba(build_X(master))[:,1]\n",
    "    except: master['PED_proba_refined'] = np.nan\n",
    "    print(\"\\n=== After bootcamp (refined) ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(master[target], master['PED_pred_refined']))\n",
    "    print(\"Confusion:\\n\", confusion_matrix(master[target], master['PED_pred_refined']))\n",
    "    print(\"Report:\\n\", classification_report(master[target], master['PED_pred_refined'], digits=4))\n",
    "    try: print(\"ROC-AUC:\", roc_auc_score(master[target], master['PED_proba_refined']))\n",
    "    except: pass\n",
    "    to_io(master, \"master_PED_predictions_refined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9fa6dbff-0dbe-4f00-a620-059709797331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== After second bootcamp (v3) ===\n",
      "Accuracy: 0.9974883712887551\n",
      "Confusion:\n",
      " [[452120      0]\n",
      " [  1162   9366]]\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9974    1.0000    0.9987    452120\n",
      "           1     1.0000    0.8896    0.9416     10528\n",
      "\n",
      "    accuracy                         0.9975    462648\n",
      "   macro avg     0.9987    0.9448    0.9702    462648\n",
      "weighted avg     0.9975    0.9975    0.9974    462648\n",
      "\n",
      "ROC-AUC: 0.9818665069779668\n"
     ]
    }
   ],
   "source": [
    "hard2 = master[master[target] != master['PED_pred_refined']].copy() if 'PED_pred_refined' in master else pd.DataFrame()\n",
    "if not hard2.empty:\n",
    "    hard_ex = resample(hard2, replace=True, n_samples=len(hard2)*2, random_state=RAND_SEED)\n",
    "    corr2 = master[master[target] == master.get('PED_pred_refined', master['PED_pred'])]\n",
    "    match_corr = resample(corr2, replace=False, n_samples=min(len(corr2), len(hard_ex)), random_state=RAND_SEED)\n",
    "    boot2 = pd.concat([hard_ex, match_corr], ignore_index=True)\n",
    "    for col in features_train:\n",
    "        if np.issubdtype(master[col].dtype, np.number):\n",
    "            boot2[col] = pd.to_numeric(boot2[col], errors='coerce')\n",
    "            boot2[col] += np.random.normal(0, 0.15*(boot2[col].std()+1), size=len(boot2))\n",
    "    model.fit(build_X(boot2), boot2[target].values)\n",
    "    master['PED_pred_v3']  = model.predict(build_X(master))\n",
    "    try: master['PED_proba_v3'] = model.predict_proba(build_X(master))[:,1]\n",
    "    except: master['PED_proba_v3'] = np.nan\n",
    "    print(\"\\n=== After second bootcamp (v3) ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(master[target], master['PED_pred_v3']))\n",
    "    print(\"Confusion:\\n\", confusion_matrix(master[target], master['PED_pred_v3']))\n",
    "    print(\"Report:\\n\", classification_report(master[target], master['PED_pred_v3'], digits=4))\n",
    "    try: print(\"ROC-AUC:\", roc_auc_score(master[target], master['PED_proba_v3']))\n",
    "    except: pass\n",
    "    to_io(master, \"master_PED_predictions_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3d1c6c6d-866c-41bd-8bec-89fb36fd2213",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_master = master.copy()\n",
    "synthetic_enc = synthetic.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c89adee7-223b-4346-ab24-279f39f9607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sector_bias(df, col='category_code'):\n",
    "    if col not in df.columns: return {}\n",
    "    try:\n",
    "        uniq = pd.Series(df[col].unique()).dropna().astype(int)\n",
    "    except:\n",
    "        uniq = pd.to_numeric(pd.Series(df[col].unique()), errors='coerce').dropna().astype(int)\n",
    "    return {int(k): float(np.clip(np.random.normal(1.0,0.2),0.7,1.3)) for k in uniq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8b1bb13e-88d5-45c9-b554-34f6b62cbcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PEDAgent:\n",
    "    def __init__(self, agent_id, dna):\n",
    "        self.id = agent_id\n",
    "        self.dna = dna\n",
    "        self.model = RandomForestClassifier(\n",
    "            n_estimators=dna['n_estimators'],\n",
    "            max_depth=dna['max_depth'],\n",
    "            min_samples_split=dna['min_samples_split'],\n",
    "            class_weight='balanced',\n",
    "            random_state=RAND_SEED + agent_id,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        self.threshold = dna['threshold']\n",
    "        self.sector_bias = dna['sector_bias']\n",
    "        self.sector_col = dna.get('sector_col', 'category_code')\n",
    "    def train(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "    def predict(self, X, sectors):\n",
    "        probs = self.model.predict_proba(X)[:,1]\n",
    "        if self.sector_col in train_master.columns and self.sector_bias:\n",
    "            bias = np.array([self.sector_bias.get(int(s), 1.0) if not pd.isna(s) else 1.0 for s in sectors])\n",
    "            probs = np.clip(probs * bias, 0, 1)\n",
    "        preds = (probs >= self.threshold).astype(int)\n",
    "        return preds, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "642b97e2-8204-423d-bd2f-5f8dece6f322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_dna():\n",
    "    return {\n",
    "        'n_estimators': random.choice([50,100,150,200]),\n",
    "        'max_depth': random.choice([5,10,15,20,None]),\n",
    "        'min_samples_split': random.choice([2,4,8,16]),\n",
    "        'threshold': float(np.clip(np.random.normal(0.5,0.2),0.2,0.8)),\n",
    "        'sector_bias': build_sector_bias(train_master, 'category_code'),\n",
    "        'sector_col': 'category_code'\n",
    "    }\n",
    "def crossover(d1, d2):\n",
    "    child = {}\n",
    "    for k in d1:\n",
    "        if k == 'sector_bias':\n",
    "            keys = set(d1[k].keys()) | set(d2[k].keys())\n",
    "            child[k] = {sec: float(np.clip(np.mean([d1[k].get(sec,1.0), d2[k].get(sec,1.0)]) + np.random.normal(0,0.05), 0.7,1.3)) for sec in keys}\n",
    "        else:\n",
    "            child[k] = d1[k] if np.random.rand()<0.5 else d2[k]\n",
    "    return child\n",
    "def mutate(d):\n",
    "    if np.random.rand() < MUTATION_RATE: d['threshold'] = float(np.clip(d['threshold'] + np.random.normal(0,0.05),0.2,0.8))\n",
    "    if np.random.rand() < MUTATION_RATE: d['n_estimators'] = max(30, int(d['n_estimators'] + int(np.random.normal(0,30))))\n",
    "    if np.random.rand() < MUTATION_RATE: d['max_depth'] = None if np.random.rand()<0.2 else max(2, int(np.random.normal(10,5)))\n",
    "    if np.random.rand() < MUTATION_RATE:\n",
    "        for sec in d['sector_bias']:\n",
    "            d['sector_bias'][sec] = float(np.clip(d['sector_bias'][sec] + np.random.normal(0,0.1), 0.7,1.3))\n",
    "    return d\n",
    "def agent_fitness(df):\n",
    "    roi = (df['true_label'] * df['agent_pred']).sum()\n",
    "    regret = ((1 - df['true_label']) * df['agent_pred']).sum()\n",
    "    entropy = -np.mean(df['agent_conf'] * np.log2(df['agent_conf'] + 1e-6))\n",
    "    return float(roi - 0.5*regret + 0.1*entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a0f574ce-a703-4426-bfd3-4e01b8ebfebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [PEDAgent(i, random_dna()) for i in range(N_AGENTS)]\n",
    "failure_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c048c544-ee5e-4c7f-8958-c3797ced7b6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generation 1/10 ===\n",
      "Agent 00 | acc=0.998 | thr=0.26 | trees=50\n",
      "Agent 01 | acc=1.000 | thr=0.55 | trees=100\n",
      "Agent 02 | acc=1.000 | thr=0.52 | trees=100\n",
      "Agent 03 | acc=1.000 | thr=0.52 | trees=200\n",
      "Agent 04 | acc=1.000 | thr=0.35 | trees=200\n",
      "Agent 05 | acc=1.000 | thr=0.51 | trees=50\n",
      "Agent 06 | acc=0.999 | thr=0.37 | trees=100\n",
      "Agent 07 | acc=0.502 | thr=0.46 | trees=50\n",
      "Agent 08 | acc=0.995 | thr=0.22 | trees=150\n",
      "Agent 09 | acc=0.999 | thr=0.68 | trees=50\n",
      "Agent 10 | acc=0.998 | thr=0.69 | trees=100\n",
      "Agent 11 | acc=1.000 | thr=0.59 | trees=50\n",
      "Agent 12 | acc=0.997 | thr=0.80 | trees=150\n",
      "Agent 13 | acc=1.000 | thr=0.60 | trees=150\n",
      "Agent 14 | acc=0.998 | thr=0.68 | trees=150\n",
      "Agent 15 | acc=1.000 | thr=0.40 | trees=100\n",
      "Agent 16 | acc=0.991 | thr=0.74 | trees=50\n",
      "Agent 17 | acc=0.888 | thr=0.53 | trees=50\n",
      "Agent 18 | acc=1.000 | thr=0.32 | trees=150\n",
      "Agent 19 | acc=0.998 | thr=0.33 | trees=150\n",
      "Top agents: [6, 17, 11, 5, 2, 15]\n",
      "\n",
      "=== Generation 2/10 ===\n",
      "Agent 06 | acc=1.000 | thr=0.37 | trees=100\n",
      "Agent 17 | acc=0.998 | thr=0.53 | trees=50\n",
      "Agent 11 | acc=0.924 | thr=0.59 | trees=50\n",
      "Agent 05 | acc=0.998 | thr=0.51 | trees=50\n",
      "Agent 02 | acc=1.000 | thr=0.52 | trees=100\n",
      "Agent 15 | acc=0.999 | thr=0.40 | trees=100\n",
      "Agent 20 | acc=1.000 | thr=0.52 | trees=100\n",
      "Agent 21 | acc=1.000 | thr=0.53 | trees=100\n",
      "Agent 22 | acc=0.869 | thr=0.54 | trees=50\n",
      "Agent 23 | acc=0.999 | thr=0.36 | trees=100\n",
      "Agent 24 | acc=0.999 | thr=0.50 | trees=50\n",
      "Agent 25 | acc=0.999 | thr=0.54 | trees=100\n",
      "Agent 26 | acc=1.000 | thr=0.53 | trees=50\n",
      "Agent 27 | acc=0.863 | thr=0.67 | trees=100\n",
      "Agent 28 | acc=0.999 | thr=0.65 | trees=100\n",
      "Agent 29 | acc=1.000 | thr=0.52 | trees=100\n",
      "Agent 30 | acc=1.000 | thr=0.40 | trees=100\n",
      "Agent 31 | acc=0.999 | thr=0.40 | trees=100\n",
      "Agent 32 | acc=1.000 | thr=0.59 | trees=100\n",
      "Agent 33 | acc=1.000 | thr=0.37 | trees=100\n",
      "Top agents: [0, 1, 2, 3, 4, 5]\n",
      "\n",
      "=== Generation 3/10 ===\n",
      "Agent 06 | acc=0.999 | thr=0.37 | trees=100\n",
      "Agent 17 | acc=0.879 | thr=0.53 | trees=50\n",
      "Agent 11 | acc=1.000 | thr=0.59 | trees=50\n",
      "Agent 05 | acc=0.999 | thr=0.51 | trees=50\n",
      "Agent 02 | acc=0.988 | thr=0.52 | trees=100\n",
      "Agent 15 | acc=1.000 | thr=0.40 | trees=100\n",
      "Agent 20 | acc=0.855 | thr=0.52 | trees=33\n",
      "Agent 21 | acc=1.000 | thr=0.52 | trees=62\n",
      "Agent 22 | acc=1.000 | thr=0.47 | trees=100\n",
      "Agent 23 | acc=1.000 | thr=0.52 | trees=100\n",
      "Agent 24 | acc=0.910 | thr=0.59 | trees=100\n",
      "Agent 25 | acc=0.888 | thr=0.53 | trees=100\n",
      "Agent 26 | acc=1.000 | thr=0.37 | trees=134\n",
      "Agent 27 | acc=0.909 | thr=0.59 | trees=98\n",
      "Agent 28 | acc=1.000 | thr=0.40 | trees=100\n",
      "Agent 29 | acc=1.000 | thr=0.37 | trees=45\n",
      "Agent 30 | acc=0.999 | thr=0.37 | trees=50\n",
      "Agent 31 | acc=1.000 | thr=0.51 | trees=50\n",
      "Agent 32 | acc=1.000 | thr=0.59 | trees=50\n",
      "Agent 33 | acc=1.000 | thr=0.42 | trees=50\n",
      "Top agents: [0, 1, 3, 4, 6, 5]\n",
      "\n",
      "=== Generation 4/10 ===\n",
      "Agent 06 | acc=1.000 | thr=0.37 | trees=100\n",
      "Agent 17 | acc=0.999 | thr=0.53 | trees=50\n",
      "Agent 05 | acc=1.000 | thr=0.51 | trees=50\n",
      "Agent 02 | acc=1.000 | thr=0.52 | trees=100\n",
      "Agent 20 | acc=0.843 | thr=0.52 | trees=33\n",
      "Agent 15 | acc=1.000 | thr=0.40 | trees=100\n",
      "Agent 20 | acc=1.000 | thr=0.40 | trees=50\n",
      "Agent 21 | acc=0.999 | thr=0.53 | trees=50\n",
      "Agent 22 | acc=1.000 | thr=0.37 | trees=100\n",
      "Agent 23 | acc=0.999 | thr=0.45 | trees=50\n",
      "Agent 24 | acc=0.991 | thr=0.53 | trees=100\n",
      "Agent 25 | acc=0.999 | thr=0.52 | trees=33\n",
      "Agent 26 | acc=1.000 | thr=0.44 | trees=50\n",
      "Agent 27 | acc=1.000 | thr=0.40 | trees=100\n",
      "Agent 28 | acc=1.000 | thr=0.37 | trees=100\n",
      "Agent 29 | acc=1.000 | thr=0.37 | trees=100\n",
      "Agent 30 | acc=0.998 | thr=0.51 | trees=50\n",
      "Agent 31 | acc=0.975 | thr=0.52 | trees=33\n",
      "Agent 32 | acc=0.876 | thr=0.52 | trees=33\n",
      "Agent 33 | acc=1.000 | thr=0.43 | trees=100\n",
      "Top agents: [0, 1, 2, 3, 4, 6]\n",
      "\n",
      "=== Generation 5/10 ===\n",
      "Agent 06 | acc=1.000 | thr=0.37 | trees=100\n",
      "Agent 17 | acc=0.875 | thr=0.53 | trees=50\n",
      "Agent 05 | acc=1.000 | thr=0.51 | trees=50\n",
      "Agent 02 | acc=0.989 | thr=0.52 | trees=100\n",
      "Agent 20 | acc=0.853 | thr=0.52 | trees=33\n",
      "Agent 20 | acc=1.000 | thr=0.40 | trees=50\n",
      "Agent 20 | acc=1.000 | thr=0.52 | trees=67\n",
      "Agent 21 | acc=0.882 | thr=0.53 | trees=33\n",
      "Agent 22 | acc=1.000 | thr=0.60 | trees=30\n",
      "Agent 23 | acc=1.000 | thr=0.52 | trees=50\n",
      "Agent 24 | acc=1.000 | thr=0.52 | trees=50\n",
      "Agent 25 | acc=0.227 | thr=0.40 | trees=50\n",
      "Agent 26 | acc=1.000 | thr=0.52 | trees=100\n",
      "Agent 27 | acc=1.000 | thr=0.52 | trees=50\n",
      "Agent 28 | acc=1.000 | thr=0.52 | trees=78\n",
      "Agent 29 | acc=0.999 | thr=0.52 | trees=33\n",
      "Agent 30 | acc=1.000 | thr=0.52 | trees=100\n",
      "Agent 31 | acc=1.000 | thr=0.40 | trees=50\n",
      "Agent 32 | acc=1.000 | thr=0.52 | trees=30\n",
      "Agent 33 | acc=0.998 | thr=0.40 | trees=131\n",
      "Top agents: [0, 1, 2, 3, 4, 6]\n",
      "\n",
      "=== Generation 6/10 ===\n",
      "Agent 06 | acc=1.000 | thr=0.37 | trees=100\n",
      "Agent 17 | acc=0.999 | thr=0.53 | trees=50\n",
      "Agent 05 | acc=1.000 | thr=0.51 | trees=50\n",
      "Agent 02 | acc=0.991 | thr=0.52 | trees=100\n",
      "Agent 20 | acc=0.999 | thr=0.52 | trees=33\n",
      "Agent 20 | acc=1.000 | thr=0.52 | trees=67\n",
      "Agent 20 | acc=0.999 | thr=0.53 | trees=33\n",
      "Agent 21 | acc=1.000 | thr=0.52 | trees=33\n",
      "Agent 22 | acc=0.998 | thr=0.51 | trees=50\n",
      "Agent 23 | acc=1.000 | thr=0.52 | trees=33\n",
      "Agent 24 | acc=1.000 | thr=0.53 | trees=67\n",
      "Agent 25 | acc=1.000 | thr=0.52 | trees=100\n",
      "Agent 26 | acc=0.999 | thr=0.52 | trees=33\n",
      "Agent 27 | acc=1.000 | thr=0.51 | trees=74\n",
      "Agent 28 | acc=1.000 | thr=0.52 | trees=33\n",
      "Agent 29 | acc=0.991 | thr=0.53 | trees=96\n",
      "Agent 30 | acc=1.000 | thr=0.51 | trees=50\n",
      "Agent 31 | acc=0.890 | thr=0.53 | trees=50\n",
      "Agent 32 | acc=1.000 | thr=0.52 | trees=100\n",
      "Agent 33 | acc=1.000 | thr=0.52 | trees=50\n",
      "Top agents: [0, 1, 2, 3, 4, 5]\n",
      "\n",
      "=== Generation 7/10 ===\n",
      "Agent 06 | acc=1.000 | thr=0.37 | trees=100\n",
      "Agent 17 | acc=0.999 | thr=0.53 | trees=50\n",
      "Agent 05 | acc=1.000 | thr=0.51 | trees=50\n",
      "Agent 02 | acc=0.990 | thr=0.52 | trees=100\n",
      "Agent 20 | acc=0.997 | thr=0.52 | trees=33\n",
      "Agent 20 | acc=1.000 | thr=0.52 | trees=67\n",
      "Agent 20 | acc=1.000 | thr=0.37 | trees=50\n",
      "Agent 21 | acc=1.000 | thr=0.52 | trees=67\n",
      "Agent 22 | acc=0.999 | thr=0.52 | trees=100\n",
      "Agent 23 | acc=1.000 | thr=0.52 | trees=100\n",
      "Agent 24 | acc=0.885 | thr=0.53 | trees=100\n",
      "Agent 25 | acc=1.000 | thr=0.52 | trees=50\n",
      "Agent 26 | acc=0.888 | thr=0.52 | trees=46\n",
      "Agent 27 | acc=0.998 | thr=0.37 | trees=100\n",
      "Agent 28 | acc=1.000 | thr=0.51 | trees=50\n",
      "Agent 29 | acc=0.999 | thr=0.54 | trees=33\n",
      "Agent 30 | acc=1.000 | thr=0.52 | trees=134\n",
      "Agent 31 | acc=0.998 | thr=0.53 | trees=70\n",
      "Agent 32 | acc=1.000 | thr=0.52 | trees=100\n",
      "Agent 33 | acc=1.000 | thr=0.56 | trees=100\n",
      "Top agents: [0, 1, 2, 3, 4, 6]\n",
      "\n",
      "=== Generation 8/10 ===\n",
      "Agent 06 | acc=1.000 | thr=0.37 | trees=100\n",
      "Agent 17 | acc=0.999 | thr=0.53 | trees=50\n",
      "Agent 05 | acc=1.000 | thr=0.51 | trees=50\n",
      "Agent 02 | acc=1.000 | thr=0.52 | trees=100\n",
      "Agent 20 | acc=1.000 | thr=0.52 | trees=33\n",
      "Agent 20 | acc=0.999 | thr=0.37 | trees=50\n",
      "Agent 20 | acc=0.999 | thr=0.53 | trees=100\n",
      "Agent 21 | acc=1.000 | thr=0.34 | trees=100\n",
      "Agent 22 | acc=0.989 | thr=0.52 | trees=100\n",
      "Agent 23 | acc=1.000 | thr=0.52 | trees=100\n",
      "Agent 24 | acc=0.993 | thr=0.52 | trees=100\n",
      "Agent 25 | acc=1.000 | thr=0.53 | trees=100\n",
      "Agent 26 | acc=1.000 | thr=0.51 | trees=100\n",
      "Agent 27 | acc=1.000 | thr=0.52 | trees=50\n",
      "Agent 28 | acc=1.000 | thr=0.52 | trees=100\n",
      "Agent 29 | acc=0.999 | thr=0.37 | trees=50\n",
      "Agent 30 | acc=1.000 | thr=0.37 | trees=50\n",
      "Agent 31 | acc=0.998 | thr=0.53 | trees=50\n",
      "Agent 32 | acc=0.992 | thr=0.53 | trees=50\n",
      "Agent 33 | acc=0.998 | thr=0.53 | trees=50\n",
      "Top agents: [0, 1, 2, 3, 4, 6]\n",
      "\n",
      "=== Generation 9/10 ===\n",
      "Agent 06 | acc=1.000 | thr=0.37 | trees=100\n",
      "Agent 17 | acc=0.881 | thr=0.53 | trees=50\n",
      "Agent 05 | acc=1.000 | thr=0.51 | trees=50\n",
      "Agent 02 | acc=1.000 | thr=0.52 | trees=100\n",
      "Agent 20 | acc=0.998 | thr=0.52 | trees=33\n",
      "Agent 20 | acc=0.881 | thr=0.53 | trees=100\n",
      "Agent 20 | acc=0.999 | thr=0.55 | trees=100\n",
      "Agent 21 | acc=1.000 | thr=0.51 | trees=50\n",
      "Agent 22 | acc=0.983 | thr=0.53 | trees=100\n",
      "Agent 23 | acc=1.000 | thr=0.52 | trees=100\n",
      "Agent 24 | acc=1.000 | thr=0.37 | trees=100\n",
      "Agent 25 | acc=0.998 | thr=0.53 | trees=100\n",
      "Agent 26 | acc=1.000 | thr=0.53 | trees=100\n",
      "Agent 27 | acc=1.000 | thr=0.53 | trees=100\n",
      "Agent 28 | acc=0.753 | thr=0.37 | trees=100\n",
      "Agent 29 | acc=0.886 | thr=0.52 | trees=33\n",
      "Agent 30 | acc=1.000 | thr=0.53 | trees=100\n",
      "Agent 31 | acc=0.999 | thr=0.33 | trees=100\n",
      "Agent 32 | acc=1.000 | thr=0.52 | trees=100\n",
      "Agent 33 | acc=0.999 | thr=0.52 | trees=100\n",
      "Top agents: [0, 1, 2, 3, 4, 5]\n",
      "\n",
      "=== Generation 10/10 ===\n",
      "Agent 06 | acc=1.000 | thr=0.37 | trees=100\n",
      "Agent 17 | acc=0.882 | thr=0.53 | trees=50\n",
      "Agent 05 | acc=0.999 | thr=0.51 | trees=50\n",
      "Agent 02 | acc=1.000 | thr=0.52 | trees=100\n",
      "Agent 20 | acc=0.831 | thr=0.52 | trees=33\n",
      "Agent 20 | acc=0.893 | thr=0.53 | trees=100\n",
      "Agent 20 | acc=0.992 | thr=0.37 | trees=50\n",
      "Agent 21 | acc=1.000 | thr=0.53 | trees=100\n",
      "Agent 22 | acc=0.999 | thr=0.53 | trees=63\n",
      "Agent 23 | acc=1.000 | thr=0.51 | trees=39\n",
      "Agent 24 | acc=1.000 | thr=0.43 | trees=50\n",
      "Agent 25 | acc=0.998 | thr=0.51 | trees=50\n",
      "Agent 26 | acc=1.000 | thr=0.51 | trees=71\n",
      "Agent 27 | acc=1.000 | thr=0.41 | trees=100\n",
      "Agent 28 | acc=1.000 | thr=0.52 | trees=100\n",
      "Agent 29 | acc=1.000 | thr=0.52 | trees=153\n",
      "Agent 30 | acc=0.884 | thr=0.53 | trees=100\n",
      "Agent 31 | acc=1.000 | thr=0.37 | trees=50\n",
      "Agent 32 | acc=0.999 | thr=0.37 | trees=100\n",
      "Agent 33 | acc=0.893 | thr=0.53 | trees=100\n",
      "Top agents: [0, 1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "for gen in range(N_GENERATIONS):\n",
    "    print(f\"\\n=== Generation {gen+1}/{N_GENERATIONS} ===\")\n",
    "    actions_all = []\n",
    "    for i, agent in enumerate(agents):\n",
    "        if np.random.rand() < GA_REAL_RATIO:\n",
    "            dfb = train_master.sample(AGENT_BATCH, replace=True, random_state=gen*111 + i)\n",
    "            Xb = build_X(dfb); yb = dfb[target].values\n",
    "            sectors = dfb['category_code'].values if 'category_code' in dfb else np.full(len(dfb), np.nan)\n",
    "        else:\n",
    "            dfs = synthetic.sample(AGENT_BATCH, replace=True, random_state=gen*222 + i)\n",
    "            Xb = build_X(dfs); yb = dfs[target].values\n",
    "            sectors = dfs['category_code'].values if 'category_code' in dfs else np.full(len(dfs), np.nan)\n",
    "        agent.train(Xb, yb)\n",
    "        preds, probs = agent.predict(Xb, sectors)\n",
    "        acc = accuracy_score(yb, preds)\n",
    "        actions = pd.DataFrame({\n",
    "            'agent_id': agent.id, 'gen': gen,\n",
    "            'true_label': yb, 'agent_pred': preds, 'agent_conf': probs\n",
    "        })\n",
    "        actions['fail'] = (actions['agent_pred'] != actions['true_label']).astype(int)\n",
    "        failure_log.append(actions[actions['fail']==1].copy())\n",
    "        actions_all.append(actions)\n",
    "        print(f\"Agent {agent.id:02d} | acc={acc:.3f} | thr={agent.threshold:.2f} | trees={agent.dna['n_estimators']}\")\n",
    "    actions_all = pd.concat(actions_all, ignore_index=True)\n",
    "    fitness_scores = []\n",
    "    for aid in range(N_AGENTS):\n",
    "        f = agent_fitness(actions_all[actions_all['agent_id']==aid])\n",
    "        fitness_scores.append((aid, f))\n",
    "    fitness_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_ids = [aid for aid,_ in fitness_scores[:TOP_K]]\n",
    "    print(\"Top agents:\", top_ids)\n",
    "    parents = [agents[i] for i in top_ids]\n",
    "    children = []\n",
    "    while len(children) < N_CHILDREN:\n",
    "        p1, p2 = random.sample(parents, 2)\n",
    "        dna = mutate(crossover(p1.dna, p2.dna))\n",
    "        children.append(PEDAgent(N_AGENTS + len(children), dna))\n",
    "    agents = parents + children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "da123383-ad81-4121-8c58-2b197161876c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evolution finished ===\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Evolution finished ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0362eeb6-4e7b-4ee4-a331-c807b0c64e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    agent_id  accuracy\n",
      "19        33   0.99995\n",
      "7         21   0.99995\n",
      "16        30   0.99995\n",
      "3          2   0.99995\n",
      "13        27   0.99995\n",
      "11        25   0.99995\n",
      "6         20   0.99995\n",
      "0          6   0.99990\n",
      "2          5   0.99985\n",
      "8         22   0.99840\n"
     ]
    }
   ],
   "source": [
    "eval_df = train_master.sample(min(20000, len(train_master)), random_state=RAND_SEED)\n",
    "scores = []\n",
    "for agent in agents:\n",
    "    agent.train(build_X(eval_df), eval_df[target].values)\n",
    "    pr, _ = agent.predict(build_X(eval_df), eval_df['category_code'].values if 'category_code' in eval_df else np.full(len(eval_df), np.nan))\n",
    "    scores.append({'agent_id': agent.id, 'accuracy': accuracy_score(eval_df[target].values, pr)})\n",
    "print(pd.DataFrame(scores).sort_values('accuracy', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7d93627a-dd94-43c7-ad8f-be26b84d2c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Meta-Model Eval ===\n",
      "Accuracy: 0.9989\n",
      "Confusion:\n",
      " [[9798    0]\n",
      " [  11  191]]\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9989    1.0000    0.9994      9798\n",
      "           1     1.0000    0.9455    0.9720       202\n",
      "\n",
      "    accuracy                         0.9989     10000\n",
      "   macro avg     0.9994    0.9728    0.9857     10000\n",
      "weighted avg     0.9989    0.9989    0.9989     10000\n",
      "\n",
      "ROC-AUC: 0.9999982316051568\n"
     ]
    }
   ],
   "source": [
    "if len(failure_log):\n",
    "    failures_df = pd.concat(failure_log, ignore_index=True)\n",
    "    failures_df['regret'] = np.abs(failures_df['true_label'] - failures_df['agent_pred'])\n",
    "    failures_df['decision_entropy'] = -failures_df['agent_conf'] * np.log2(failures_df['agent_conf'] + 1e-6)\n",
    "    worst = failures_df[failures_df['regret']==1]\n",
    "    boot = resample(worst, replace=True, n_samples=len(worst), random_state=RAND_SEED)\n",
    "    meta_fail = pd.concat([failures_df, boot], ignore_index=True)\n",
    "    normal_n = min(N_META, len(train_master))\n",
    "    normal = train_master.sample(normal_n, replace=True, random_state=123).copy()\n",
    "    normal['agent_conf'] = 0.5\n",
    "    normal['decision_entropy'] = -0.5*np.log2(0.5+1e-6)\n",
    "    normal['true_label'] = normal[target]\n",
    "    meta_train = meta_fail.copy()\n",
    "    meta_features = features_train + ['agent_conf','decision_entropy']\n",
    "    filler = train_master.sample(len(meta_train), replace=True, random_state=RAND_SEED)[features_train].reset_index(drop=True)\n",
    "    meta_train = pd.concat([meta_train.reset_index(drop=True), filler], axis=1)\n",
    "    normal_meta = pd.concat([normal[['agent_conf','decision_entropy']].reset_index(drop=True),\n",
    "                             normal[features_train].reset_index(drop=True)], axis=1)\n",
    "    normal_meta['true_label'] = normal[target].values\n",
    "    meta_train = pd.concat([meta_train, normal_meta], ignore_index=True)\n",
    "    y_meta = meta_train['true_label'].astype(int).values\n",
    "    X_meta = meta_train[meta_features].values\n",
    "    meta_model = RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=RAND_SEED, n_jobs=-1)\n",
    "    meta_model.fit(X_meta, y_meta)\n",
    "    hold = train_master.sample(min(10000, len(train_master)), random_state=1234).copy()\n",
    "    hold['agent_conf'] = 0.5\n",
    "    hold['decision_entropy'] = -0.5*np.log2(0.5+1e-6)\n",
    "    X_hold = np.hstack([hold[features_train].values,\n",
    "                        hold[['agent_conf','decision_entropy']].values])\n",
    "    y_hold = hold[target].astype(int).values\n",
    "    y_pred_meta = meta_model.predict(X_hold)\n",
    "    y_proba_meta = meta_model.predict_proba(X_hold)[:,1]\n",
    "    print(\"\\n=== Meta-Model Eval ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_hold, y_pred_meta))\n",
    "    print(\"Confusion:\\n\", confusion_matrix(y_hold, y_pred_meta))\n",
    "    print(\"Report:\\n\", classification_report(y_hold, y_pred_meta, digits=4))\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y_hold, y_proba_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "07b96b3b-e015-4b58-bd09-c1f6b1fcf41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DONE] All stages executed. Nothing is missing. If you want me to toggle OHE/Parquet/Selenium on, flip the flags up top.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[DONE] All stages executed. Nothing is missing. If you want me to toggle OHE/Parquet/Selenium on, flip the flags up top.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "1d1ef3fa-4803-4e71-a7f5-144ffa3c6e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pathlib, re, time, random, requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "c0aa4d59-6939-42bb-a681-3b6aec4859c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"27569da692c106a1aef7f42f7fbca08e2b0b849d\"] = SERPER_API_KEY\n",
    "ENABLE_SERPER = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "c66f4d6f-8839-42bf-9343-c788198d3d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENABLE_SERPER = True | key_len = 40\n"
     ]
    }
   ],
   "source": [
    "print(\"ENABLE_SERPER =\", ENABLE_SERPER, \"| key_len =\", len(SERPER_API_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "8c952935-7a27-4378-a19d-01642fcee392",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERPER_API_KEY = (os.getenv(\"SERPER_API_KEY\", \"\").strip()\n",
    "                  or os.getenv(\"27569da692c106a1aef7f42f7fbca08e2b0b849d\", \"\").strip())\n",
    "ENABLE_SERPER = bool(SERPER_API_KEY)\n",
    "SERPER_ENDPOINT = \"https://google.serper.dev/news\"\n",
    "SERPER_TIMEOUT  = 12\n",
    "SERPER_SLEEP_BASE = 0.12\n",
    "DEBUG_SERPER = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "2c18b110-b7d9-4fb0-8f3f-85876a045441",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SERPER_BUDGET_TOTAL = 580\n",
    "TEST_SERPER_BUDGET_TOTAL  = 1200\n",
    "SERPER_PER_AGENT_PER_GEN   = 12  \n",
    "FORCE_MIN_ENRICH_PER_AGENT = 8    \n",
    "SERPER_WINDOW_7D  = 7\n",
    "SERPER_WINDOW_30D = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "85d791c5-b212-4657-9944-f4e2d1eddc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: OUT_ROOT\n",
    "except NameError: OUT_ROOT = \".\"\n",
    "CACHE_PATH = os.path.join(OUT_ROOT, \"serper_cache.jsonl\")\n",
    "_path = pathlib.Path(CACHE_PATH); _path.parent.mkdir(parents=True, exist_ok=True)\n",
    "if not _path.exists(): _path.write_text(\"\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "773ae67c-f25d-45da-b05e-77ebbf4c4791",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _SerperCache:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.mem = {}\n",
    "        try:\n",
    "            with open(self.path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    if not line.strip(): continue\n",
    "                    obj = json.loads(line)\n",
    "                    key = (obj[\"q\"], obj.get(\"hl\",\"\"), obj.get(\"gl\",\"\"))\n",
    "                    self.mem[key] = obj[\"payload\"]  \n",
    "        except Exception:\n",
    "            pass\n",
    "    def get(self, q, hl=\"en\", gl=\"us\"):\n",
    "        return self.mem.get((q, hl, gl))\n",
    "    def put(self, q, payload, hl=\"en\", gl=\"us\"):\n",
    "        self.mem[(q, hl, gl)] = payload\n",
    "        rec = {\"q\": q, \"hl\": hl, \"gl\": gl, \"payload\": payload, \"ts\": pd.Timestamp.now(tz=\"UTC\").isoformat()}\n",
    "        with open(self.path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(rec) + \"\\n\")\n",
    "_SERP_CACHE = _SerperCache(CACHE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "e2fbcd60-ce88-4fec-ad22-112d1e1e0d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "_session2 = requests.Session()\n",
    "_session2.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0 (PEDAgents/1.0)\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "})\n",
    "def _sleep_jitter():\n",
    "    time.sleep(SERPER_SLEEP_BASE + random.random()*0.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "70b9bb1f-e16e-4797-86ba-64f4ae0c6e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_rel_timedelta(unit: str, val: int):\n",
    "    CAPS = {\"minute\": 60*24*30, \"hour\": 24*365*5, \"day\": 365*10, \"week\": 52*10, \"month\": 12*10, \"year\": 50}\n",
    "    unit = str(unit).lower().rstrip(\"s\")\n",
    "    if unit not in CAPS: return None\n",
    "    try: val = int(val)\n",
    "    except: return None\n",
    "    val = max(0, min(val, CAPS[unit]))\n",
    "    if unit == \"minute\": return pd.Timedelta(minutes=val)\n",
    "    if unit == \"hour\":   return pd.Timedelta(hours=val)\n",
    "    if unit == \"day\":    return pd.Timedelta(days=val)\n",
    "    if unit == \"week\":   return pd.Timedelta(weeks=val)\n",
    "    if unit == \"month\":  return pd.Timedelta(days=30*val)\n",
    "    if unit == \"year\":   return pd.Timedelta(days=365*val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "6face832-fd44-4a05-a063-26dd4ab24527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _norm_dt(x):\n",
    "    if not x: return pd.NaT\n",
    "    s = str(x).strip()\n",
    "    dtp = pd.to_datetime(s, errors=\"coerce\", utc=True)\n",
    "    if pd.notna(dtp): return dtp\n",
    "    matches = re.findall(r\"(\\d+)\\s+(minutes?|hours?|days?|weeks?|months?|years?)\", s, flags=re.I)\n",
    "    if not matches: return pd.NaT\n",
    "    now = pd.Timestamp.now(tz=\"UTC\")\n",
    "    best = None\n",
    "    for val_str, unit in matches:\n",
    "        delta = _safe_rel_timedelta(unit, val_str)\n",
    "        if delta is None: continue\n",
    "        cand = now - delta\n",
    "        best = cand if (best is None or cand > best) else best\n",
    "    return best if best is not None else pd.NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "f023abc3-ec2e-40ae-8ef5-886f2eb590b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_re_funding  = re.compile(r\"\\b(series [abc]|series_[abc]|seed|pre-?seed|raised|funding|round|venture|investment|ipo|acqui(?:re|sition))\\b\", re.I)\n",
    "_re_negative = re.compile(r\"\\b(layoff|lawsuit|fraud|investigation|scandal|SEC|recall|breach|hack|bankrupt|shutdown|downturn|loss)\\b\", re.I)\n",
    "_re_ai       = re.compile(r\"\\b(ai|genai|machine learning|llm|transformer|deep learning|model)\\b\", re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "66868c08-92d2-4ce0-96a8-5a323f841522",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _BudgetPool:\n",
    "    def __init__(self, total, per_agent_per_gen=0):\n",
    "        self.total = int(total)\n",
    "        self.per_agent_per_gen = int(per_agent_per_gen)\n",
    "        self.used_total = 0\n",
    "        self.used = {}  \n",
    "    def can_spend(self, gen=None, agent_id=None):\n",
    "        if self.used_total >= self.total: return False\n",
    "        if gen is not None and agent_id is not None and self.per_agent_per_gen > 0:\n",
    "            if self.used.get((gen, agent_id), 0) >= self.per_agent_per_gen:\n",
    "                return False\n",
    "        return True\n",
    "    def spend(self, gen=None, agent_id=None):\n",
    "        self.used_total += 1\n",
    "        if gen is not None and agent_id is not None and self.per_agent_per_gen > 0:\n",
    "            key = (gen, agent_id)\n",
    "            self.used[key] = self.used.get(key, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "755404e0-0484-488c-be7c-4999b0fb008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_BUDGETS = {\n",
    "    \"train\": _BudgetPool(TRAIN_SERPER_BUDGET_TOTAL, per_agent_per_gen=SERPER_PER_AGENT_PER_GEN),\n",
    "    \"test\":  _BudgetPool(TEST_SERPER_BUDGET_TOTAL,  per_agent_per_gen=0),\n",
    "}\n",
    "def _cache_has(name, hl=\"en\", gl=\"us\"):\n",
    "    q = f'\"{name}\"'\n",
    "    return _SERP_CACHE.get(q, hl, gl) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "b2a2fc02-2188-41b6-a69f-40d5b6aa6a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serper_news(company_name, lang=\"en\", country=\"us\", num=10, budget_name=None, gen=None, agent_id=None, force_min=False):\n",
    "    if not ENABLE_SERPER or not company_name or not str(company_name).strip():\n",
    "        return {\"news\": [], \"_cache\": False, \"_spent\": False}\n",
    "    q = f'\"{company_name}\"'\n",
    "    cached_payload = _SERP_CACHE.get(q, lang, country)\n",
    "    if cached_payload is not None:\n",
    "        out = dict(cached_payload)\n",
    "        out[\"_cache\"] = True; out[\"_spent\"] = False\n",
    "        return out\n",
    "    if budget_name not in _BUDGETS:\n",
    "        return {\"news\": [], \"_cache\": False, \"_spent\": False}\n",
    "    pool = _BUDGETS[budget_name]\n",
    "    if not pool.can_spend(gen, agent_id):\n",
    "        if budget_name == \"train\" and force_min and pool.used_total < pool.total:\n",
    "            pool.spend(gen, agent_id)\n",
    "        else:\n",
    "            return {\"news\": [], \"_cache\": False, \"_spent\": False}\n",
    "    else:\n",
    "        pool.spend(gen, agent_id)\n",
    "    headers = {\"X-API-KEY\": SERPER_API_KEY, \"Content-Type\": \"application/json\"}\n",
    "    payload = {\"q\": q, \"hl\": lang, \"gl\": country, \"num\": num}\n",
    "    try:\n",
    "        r = _session2.post(SERPER_ENDPOINT, headers=headers, json=payload, timeout=SERPER_TIMEOUT)\n",
    "        data = r.json() if r.status_code == 200 else {\"news\": []}\n",
    "    except Exception:\n",
    "        data = {\"news\": []}\n",
    "    _SERP_CACHE.put(q, data, lang, country)\n",
    "    _sleep_jitter()\n",
    "    out = dict(data)\n",
    "    out[\"_cache\"] = False; out[\"_spent\"] = True\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "7ab8fc4d-dc45-43b8-ba6f-c2b7ce1531fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_serper_features(company_name, **kw):\n",
    "    data = serper_news(company_name, **kw)\n",
    "    items = data.get(\"news\", []) or []\n",
    "    now = pd.Timestamp.now(tz=\"UTC\")\n",
    "    rows = []\n",
    "    for it in items:\n",
    "        try:\n",
    "            title   = it.get(\"title\",\"\") or \"\"\n",
    "            snippet = it.get(\"snippet\",\"\") or it.get(\"description\",\"\") or \"\"\n",
    "            source  = (it.get(\"source\",\"\") or \"\").strip().lower()\n",
    "            dtp     = _norm_dt(it.get(\"date\") or it.get(\"datePublished\"))\n",
    "            text    = f\"{title} {snippet}\"\n",
    "            rows.append({\n",
    "                \"source\": source,\n",
    "                \"dt\": dtp if pd.notna(dtp) else pd.NaT,\n",
    "                \"has_funding\":  bool(_re_funding.search(text)),\n",
    "                \"has_negative\": bool(_re_negative.search(text)),\n",
    "                \"has_ai\":       bool(_re_ai.search(text)),\n",
    "            })\n",
    "        except Exception:\n",
    "            continue\n",
    "    if not rows:\n",
    "        return {\n",
    "            \"serper_news_count_7d\": 0,\n",
    "            \"serper_news_count_30d\": 0,\n",
    "            \"serper_latest_hours\": 1e6,\n",
    "            \"serper_flag_funding\": 0,\n",
    "            \"serper_flag_negative\": 0,\n",
    "            \"serper_flag_ai\": 0,\n",
    "            \"serper_source_diversity\": 0,\n",
    "        }\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"dt\"] = pd.to_datetime(df[\"dt\"], errors=\"coerce\", utc=True)\n",
    "    cutoff7  = now - pd.Timedelta(days=SERPER_WINDOW_7D)\n",
    "    cutoff30 = now - pd.Timedelta(days=SERPER_WINDOW_30D)\n",
    "    n7  = int((df[\"dt\"] >= cutoff7).sum())\n",
    "    n30 = int((df[\"dt\"] >= cutoff30).sum())\n",
    "    latest_dt = df[\"dt\"].max()\n",
    "    latest_hours = float((now - latest_dt).total_seconds()/3600.0) if pd.notna(latest_dt) else 1e6\n",
    "    return {\n",
    "        \"serper_news_count_7d\": n7,\n",
    "        \"serper_news_count_30d\": n30,\n",
    "        \"serper_latest_hours\": max(0.0, latest_hours),\n",
    "        \"serper_flag_funding\":  int(df[\"has_funding\"].any()),\n",
    "        \"serper_flag_negative\": int(df[\"has_negative\"].any()),\n",
    "        \"serper_flag_ai\":       int(df[\"has_ai\"].any()),\n",
    "        \"serper_source_diversity\": int(df[\"source\"].nunique()),\n",
    "    }\n",
    "SERPER_FEATS = [\n",
    "    \"serper_news_count_7d\",\"serper_news_count_30d\",\"serper_latest_hours\",\n",
    "    \"serper_flag_funding\",\"serper_flag_negative\",\"serper_flag_ai\",\"serper_source_diversity\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "8ff2b262-bd78-459e-bc04-4e1cd31fa549",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: train_master\n",
    "except NameError: train_master = pd.DataFrame()\n",
    "try: synthetic\n",
    "except NameError: synthetic = pd.DataFrame()\n",
    "try: features_train\n",
    "except NameError: features_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "436d1d9a-882b-48ac-909b-4bcb2d89cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in SERPER_FEATS:\n",
    "    if not train_master.empty and c not in train_master.columns: train_master[c] = 0\n",
    "    if not synthetic.empty and c not in synthetic.columns:       synthetic[c]    = 0\n",
    "    if c not in features_train: features_train.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "a7676301-6223-4ac6-b7f1-e154249bb6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_X_with_serper(df):\n",
    "    for c in SERPER_FEATS:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0)\n",
    "        else:\n",
    "            df[c] = 0\n",
    "    return build_X(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "2e0172f5-5a24-4048-a5dc-21898c75b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_names(df):\n",
    "    if 'name' not in df.columns: return []\n",
    "    s = df['name'].astype(str).str.strip()\n",
    "    s = s[s != \"\"]\n",
    "    return list(pd.Series(s).drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "6fe6849d-4916-424e-b9f1-b8f9f77da84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_global_train_names = _clean_names(train_master)\n",
    "_global_test_names  = [n for n in _global_train_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "69db5c63-cf62-4e98-93ed-25aa4ebbc5dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.Random(123).shuffle(_global_train_names)\n",
    "random.Random(456).shuffle(_global_test_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "dbd78204-23e3-42ac-b4f1-70f4a24fb032",
   "metadata": {},
   "outputs": [],
   "source": [
    "_seen_train = set()   \n",
    "_seen_test  = set()   \n",
    "_enriched_train_names = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "ffb98fb7-f00b-45cd-9f6c-20c8859b3f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _next_fresh_names(pool, want, exclude=set()):\n",
    "    \"\"\"\n",
    "    Pull up to `want` unique names from the given pool that are:\n",
    "    - not in exclude\n",
    "    - not in cache\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for n in pool:\n",
    "        if n in exclude: continue\n",
    "        if _cache_has(n): continue\n",
    "        out.append(n)\n",
    "        if len(out) >= want:\n",
    "            break\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "e081a3ea-6b77-48df-9ac6-32f7acd77754",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_GENERATIONS_RT = min(3, N_GENERATIONS) if 'N_GENERATIONS' in globals() else 3\n",
    "AGENT_BATCH_RT   = min(4000, AGENT_BATCH) if 'AGENT_BATCH' in globals() else 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "1adeefdf-d97f-462d-b837-3c9930c2a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_feats_rowwise(dfb, idx, feats):\n",
    "    for k, v in feats.items():\n",
    "        if k in SERPER_FEATS:\n",
    "            dfb.loc[dfb.index[idx], k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "1f5a774a-e6e8-418c-bc2f-807d2a925f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_with_serper_pool(dfb: pd.DataFrame, gen: int, agent_id: int, pool=\"train\", name_col=\"name\"):\n",
    "    \"\"\"Budget-maximizing enrichment: use df batch names first; if not enough, pull from global pool.\"\"\"\n",
    "    for c in SERPER_FEATS:\n",
    "        if c not in dfb.columns: dfb[c] = 0\n",
    "    if not ENABLE_SERPER or name_col not in dfb.columns:\n",
    "        return dfb, 0\n",
    "    remaining = _BUDGETS[pool].total - _BUDGETS[pool].used_total\n",
    "    target_per_agent = SERPER_PER_AGENT_PER_GEN if pool==\"train\" else min(50, remaining) \n",
    "    target_per_agent = max(FORCE_MIN_ENRICH_PER_AGENT if pool==\"train\" else 0, target_per_agent)\n",
    "    cand = (\n",
    "        dfb[[name_col]].assign(_idx=np.arange(len(dfb)))\n",
    "        .dropna()\n",
    "    )\n",
    "    cand = cand[cand[name_col].astype(str).str.strip() != \"\"]\n",
    "    if cand.empty: cand = pd.DataFrame({name_col:[], \"_idx\":[]})\n",
    "    cand = cand.drop_duplicates(subset=[name_col])\n",
    "    cand = cand.sample(frac=1.0, random_state=agent_id + 991) if len(cand)>0 else cand\n",
    "    filled = 0\n",
    "    def _try_name(comp, idx=None):\n",
    "        nonlocal filled\n",
    "        kwargs = {\n",
    "            \"budget_name\": pool,\n",
    "            \"gen\": gen if pool==\"train\" else None,\n",
    "            \"agent_id\": agent_id if pool==\"train\" else None,\n",
    "            \"force_min\": (pool==\"train\" and filled < FORCE_MIN_ENRICH_PER_AGENT)\n",
    "        }\n",
    "        data = serper_news(comp, **kwargs)\n",
    "        if data.get(\"_spent\"):\n",
    "            if pool == \"train\":\n",
    "                _seen_train.add(comp); _enriched_train_names.add(comp.lower())\n",
    "            else:\n",
    "                _seen_test.add(comp)\n",
    "        feats = extract_serper_features(comp, budget_name=None)  \n",
    "        if idx is not None:\n",
    "            _apply_feats_rowwise(dfb, idx, feats)\n",
    "        filled += 1\n",
    "    for _, row in cand.iterrows():\n",
    "        if filled >= target_per_agent: break\n",
    "        if _BUDGETS[pool].used_total >= _BUDGETS[pool].total: break\n",
    "        comp = str(row[name_col]).strip()\n",
    "        if comp == \"\": continue\n",
    "        if _cache_has(comp):  \n",
    "            continue\n",
    "        _try_name(comp, int(row[\"_idx\"]))\n",
    "    if filled < target_per_agent and _BUDGETS[pool].used_total < _BUDGETS[pool].total:\n",
    "        need = min(target_per_agent - filled, _BUDGETS[pool].total - _BUDGETS[pool].used_total)\n",
    "        global_pool = _global_train_names if pool==\"train\" else _global_test_names\n",
    "        exclude = _seen_train if pool==\"train\" else _seen_test\n",
    "        fresh = _next_fresh_names(global_pool, need, exclude=exclude)\n",
    "        for comp in fresh:\n",
    "            if filled >= target_per_agent: break\n",
    "            if _BUDGETS[pool].used_total >= _BUDGETS[pool].total: break\n",
    "            _try_name(comp, None)  \n",
    "    return dfb, filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "1c2d50f5-7f5a-440c-a5cc-47f30d2cb9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Real-Time PED with Serper (TRAIN, target ~600) ===\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Real-Time PED with Serper (TRAIN, target ~600) ===\")\n",
    "try:\n",
    "    _agents_rt = agents\n",
    "except NameError:\n",
    "    _agents_rt = [PEDAgent(i, random_dna()) for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "79d0e93d-ed9d-48db-98e3-ce6646a24b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RT/TRAIN] Gen 1/3 | Train budget used: 0/580\n",
      "[RT/TRAIN] Gen 2/3 | Train budget used: 232/580\n",
      "[RT/TRAIN] Gen 3/3 | Train budget used: 464/580\n"
     ]
    }
   ],
   "source": [
    "for gen in range(N_GENERATIONS_RT):\n",
    "    print(f\"[RT/TRAIN] Gen {gen+1}/{N_GENERATIONS_RT} | Train budget used: {_BUDGETS['train'].used_total}/{_BUDGETS['train'].total}\")\n",
    "    for i, agent in enumerate(_agents_rt):\n",
    "        if 'GA_REAL_RATIO' in globals() and np.random.rand() < GA_REAL_RATIO and len(train_master) > 0:\n",
    "            dfb = train_master.sample(AGENT_BATCH_RT, replace=True, random_state=gen*111 + i).copy()\n",
    "        else:\n",
    "            dfb = synthetic.sample(AGENT_BATCH_RT, replace=True, random_state=gen*222 + i).copy()\n",
    "        if 'name' in dfb.columns:\n",
    "            dfb = dfb[dfb['name'].astype(str).str.strip() != \"\"].copy()\n",
    "        dfb, filled_n = enrich_with_serper_pool(dfb, gen, getattr(agent, 'id', i), pool=\"train\")\n",
    "        Xb = build_X_with_serper(dfb)\n",
    "        yb = dfb[target].values\n",
    "        sectors = dfb['category_code'].values if 'category_code' in dfb else np.full(len(dfb), np.nan)\n",
    "        agent.train(Xb, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "206bed44-e8a2-4823-8316-7fe1d13178d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RT/TRAIN DONE] Train budget used: 580/580\n"
     ]
    }
   ],
   "source": [
    "print(f\"[RT/TRAIN DONE] Train budget used: {_BUDGETS['train'].used_total}/{_BUDGETS['train'].total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "62f97166-27d8-4719-ab81-4cc32baff614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_holdout(master_df: pd.DataFrame, n=9000):\n",
    "    df = master_df.copy()\n",
    "    if 'name' in df.columns:\n",
    "        df = df[df['name'].astype(str).str.strip() != \"\"]\n",
    "        df = df[~df['name'].str.lower().isin({x.lower() for x in _enriched_train_names})]\n",
    "    if 'exit_flag' in df.columns:\n",
    "        df = df[df['exit_flag'].notnull()]\n",
    "    return df.sample(min(n, len(df)), random_state=2025).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "85d0bc54-fdc3-4394-bfde-cc256b70c934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bulk_enrich_holdout(hold: pd.DataFrame):\n",
    "    if 'name' not in hold.columns or hold.empty: return hold\n",
    "    names = hold['name'].astype(str).str.strip()\n",
    "    idx_map = names.reset_index().groupby('name')['index'].apply(list).to_dict()\n",
    "    for comp, idxs in idx_map.items():\n",
    "        if _BUDGETS[\"test\"].used_total >= _BUDGETS[\"test\"].total: break\n",
    "        if comp == \"\" or _cache_has(comp): continue\n",
    "        _ = serper_news(comp, budget_name=\"test\") \n",
    "        feats = extract_serper_features(comp)      \n",
    "        for idx in idxs:\n",
    "            for k,v in feats.items():\n",
    "                if k in SERPER_FEATS:\n",
    "                    hold.loc[idx, k] = v\n",
    "    if _BUDGETS[\"test\"].used_total < _BUDGETS[\"test\"].total:\n",
    "        need = _BUDGETS[\"test\"].total - _BUDGETS[\"test\"].used_total\n",
    "        fresh = _next_fresh_names(_global_test_names, need, exclude={n for n in names if n})\n",
    "        extra_rows = []\n",
    "        for comp in fresh:\n",
    "            if _BUDGETS[\"test\"].used_total >= _BUDGETS[\"test\"].total: break\n",
    "            _ = serper_news(comp, budget_name=\"test\") \n",
    "            feats = extract_serper_features(comp)\n",
    "            row = {c: 0 for c in SERPER_FEATS}\n",
    "            row.update({k:v for k,v in feats.items() if k in SERPER_FEATS})\n",
    "            row['name'] = comp\n",
    "            row['category_code'] = hold['category_code'].mode().iloc[0] if 'category_code' in hold.columns and not hold['category_code'].dropna().empty else 0\n",
    "            row['exit_flag'] = 0 if 'exit_flag' not in hold.columns else hold['exit_flag'].mode().iloc[0]\n",
    "            extra_rows.append(row)\n",
    "        if extra_rows:\n",
    "            hold = pd.concat([hold, pd.DataFrame(extra_rows)], ignore_index=True)\n",
    "    for c in SERPER_FEATS:\n",
    "        if c not in hold.columns: hold[c] = 0\n",
    "        hold[c] = pd.to_numeric(hold[c], errors='coerce').fillna(0)\n",
    "    return hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "62efe6e2-3ed1-463e-a655-b66031fcff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agents_with_serper_holdout(master_df, agents_list, n=9000):\n",
    "    hold = _make_holdout(master_df, n=n)\n",
    "    hold = _bulk_enrich_holdout(hold)  \n",
    "    X_hold = build_X_with_serper(hold)\n",
    "    y_hold = hold[target].astype(int).values if 'exit_flag' in hold.columns else np.zeros(len(hold), dtype=int)\n",
    "    results = []\n",
    "    for agent in agents_list:\n",
    "        agent.train(build_X_with_serper(train_master), train_master[target].values)\n",
    "        preds, probs = agent.predict(X_hold, hold['category_code'].values if 'category_code' in hold else np.full(len(hold), np.nan))\n",
    "        acc = accuracy_score(y_hold, preds) if len(set(y_hold))>1 else np.nan\n",
    "        auc = None\n",
    "        try:\n",
    "            auc = roc_auc_score(y_hold, probs)\n",
    "        except Exception:\n",
    "            pass\n",
    "        results.append({\"agent_id\": getattr(agent, 'id', -1), \"accuracy\": acc, \"roc_auc\": auc})\n",
    "    res_df = pd.DataFrame(results).sort_values([\"accuracy\",\"roc_auc\"], ascending=[False, False])\n",
    "    print(\"\\n=== EVAL (Hold-out + Serper) ===\")\n",
    "    print(res_df.head(10))\n",
    "    print(f\"[RT/TEST DONE] Test budget used: {_BUDGETS['test'].used_total}/{_BUDGETS['test'].total}\")\n",
    "    return res_df, hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "c051076d-b224-4c47-b241-38c05a0fdfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EVAL (Hold-out + Serper) ===\n",
      "    agent_id  accuracy   roc_auc\n",
      "16        30  1.000000  1.000000\n",
      "19        33  1.000000  1.000000\n",
      "0          6  1.000000  1.000000\n",
      "2          5  1.000000  1.000000\n",
      "7         21  1.000000  1.000000\n",
      "11        25  1.000000  1.000000\n",
      "13        27  1.000000  1.000000\n",
      "6         20  0.999778  0.999999\n",
      "3          2  0.999667  1.000000\n",
      "8         22  0.997444  0.999529\n",
      "[RT/TEST DONE] Test budget used: 1200/1200\n"
     ]
    }
   ],
   "source": [
    "eval_results, eval_hold = evaluate_agents_with_serper_holdout(train_master, _agents_rt, n=9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "90b6965e-59fc-431b-ae5a-7a035c3d5e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accs = []\n",
    "test_accs  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "5d92a13c-87b7-4d59-aca5-379f3eeac821",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RT/TRAIN] Gen 1/3 | Train budget used: 580/580\n",
      "[TRAIN] Agent 06 acc=1.000\n",
      "[TRAIN] Agent 17 acc=1.000\n",
      "[TRAIN] Agent 05 acc=1.000\n",
      "[TRAIN] Agent 02 acc=1.000\n",
      "[TRAIN] Agent 20 acc=1.000\n",
      "[TRAIN] Agent 20 acc=0.999\n",
      "[TRAIN] Agent 20 acc=1.000\n",
      "[TRAIN] Agent 21 acc=1.000\n",
      "[TRAIN] Agent 22 acc=1.000\n",
      "[TRAIN] Agent 23 acc=0.998\n",
      "[TRAIN] Agent 24 acc=0.999\n",
      "[TRAIN] Agent 25 acc=1.000\n",
      "[TRAIN] Agent 26 acc=1.000\n",
      "[TRAIN] Agent 27 acc=1.000\n",
      "[TRAIN] Agent 28 acc=0.999\n",
      "[TRAIN] Agent 29 acc=0.999\n",
      "[TRAIN] Agent 30 acc=1.000\n",
      "[TRAIN] Agent 31 acc=1.000\n",
      "[TRAIN] Agent 32 acc=1.000\n",
      "[TRAIN] Agent 33 acc=1.000\n",
      "[RT/TRAIN] Gen 2/3 | Train budget used: 580/580\n",
      "[TRAIN] Agent 06 acc=1.000\n",
      "[TRAIN] Agent 17 acc=0.999\n",
      "[TRAIN] Agent 05 acc=1.000\n",
      "[TRAIN] Agent 02 acc=1.000\n",
      "[TRAIN] Agent 20 acc=1.000\n",
      "[TRAIN] Agent 20 acc=1.000\n",
      "[TRAIN] Agent 20 acc=1.000\n",
      "[TRAIN] Agent 21 acc=1.000\n",
      "[TRAIN] Agent 22 acc=1.000\n",
      "[TRAIN] Agent 23 acc=0.997\n",
      "[TRAIN] Agent 24 acc=1.000\n",
      "[TRAIN] Agent 25 acc=1.000\n",
      "[TRAIN] Agent 26 acc=1.000\n",
      "[TRAIN] Agent 27 acc=1.000\n",
      "[TRAIN] Agent 28 acc=0.999\n",
      "[TRAIN] Agent 29 acc=1.000\n",
      "[TRAIN] Agent 30 acc=1.000\n",
      "[TRAIN] Agent 31 acc=0.999\n",
      "[TRAIN] Agent 32 acc=1.000\n",
      "[TRAIN] Agent 33 acc=1.000\n",
      "[RT/TRAIN] Gen 3/3 | Train budget used: 580/580\n",
      "[TRAIN] Agent 06 acc=1.000\n",
      "[TRAIN] Agent 17 acc=0.999\n",
      "[TRAIN] Agent 05 acc=1.000\n",
      "[TRAIN] Agent 02 acc=1.000\n",
      "[TRAIN] Agent 20 acc=1.000\n",
      "[TRAIN] Agent 20 acc=1.000\n",
      "[TRAIN] Agent 20 acc=1.000\n",
      "[TRAIN] Agent 21 acc=1.000\n",
      "[TRAIN] Agent 22 acc=0.999\n",
      "[TRAIN] Agent 23 acc=0.998\n",
      "[TRAIN] Agent 24 acc=1.000\n",
      "[TRAIN] Agent 25 acc=1.000\n",
      "[TRAIN] Agent 26 acc=1.000\n",
      "[TRAIN] Agent 27 acc=1.000\n",
      "[TRAIN] Agent 28 acc=1.000\n",
      "[TRAIN] Agent 29 acc=0.999\n",
      "[TRAIN] Agent 30 acc=1.000\n",
      "[TRAIN] Agent 31 acc=1.000\n",
      "[TRAIN] Agent 32 acc=0.999\n",
      "[TRAIN] Agent 33 acc=1.000\n"
     ]
    }
   ],
   "source": [
    "for gen in range(N_GENERATIONS_RT):\n",
    "    print(f\"[RT/TRAIN] Gen {gen+1}/{N_GENERATIONS_RT} | Train budget used: {_BUDGETS['train'].used_total}/{_BUDGETS['train'].total}\")\n",
    "    for i, agent in enumerate(_agents_rt):\n",
    "        dfb = train_master.sample(AGENT_BATCH_RT, replace=True, random_state=gen*111 + i).copy()\n",
    "        dfb = dfb[dfb['name'].astype(str).str.strip() != \"\"]\n",
    "        dfb, _ = enrich_with_serper_pool(dfb, gen, getattr(agent, 'id', i), pool=\"train\")\n",
    "        Xb = build_X_with_serper(dfb)\n",
    "        yb = dfb[target].values\n",
    "        sectors = dfb['category_code'].values if 'category_code' in dfb else np.full(len(dfb), np.nan)\n",
    "        agent.train(Xb, yb)\n",
    "        preds, _ = agent.predict(Xb, sectors)\n",
    "        acc = accuracy_score(yb, preds)\n",
    "        train_accs.append(acc)\n",
    "        print(f\"[TRAIN] Agent {getattr(agent,'id',i):02d} acc={acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "9438b996-36b7-4262-ac1c-30dd89995dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RT/TRAIN DONE] Train budget used: 580/580\n",
      "Total TRAIN accuracy: 0.9995\n"
     ]
    }
   ],
   "source": [
    "print(f\"[RT/TRAIN DONE] Train budget used: {_BUDGETS['train'].used_total}/{_BUDGETS['train'].total}\")\n",
    "print(f\"Total TRAIN accuracy: {np.mean(train_accs):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "e44956d1-f89b-4850-9749-5c9edee84054",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RT/TEST] Gen 1/3 | Test budget used: 1200/1200\n",
      "[TEST] Agent 06 acc=0.631\n",
      "[TEST] Agent 17 acc=0.803\n",
      "[TEST] Agent 05 acc=0.794\n",
      "[TEST] Agent 02 acc=0.800\n",
      "[TEST] Agent 20 acc=0.797\n",
      "[TEST] Agent 20 acc=0.823\n",
      "[TEST] Agent 20 acc=0.801\n",
      "[TEST] Agent 21 acc=0.818\n",
      "[TEST] Agent 22 acc=0.806\n",
      "[TEST] Agent 23 acc=0.549\n",
      "[TEST] Agent 24 acc=0.804\n",
      "[TEST] Agent 25 acc=0.759\n",
      "[TEST] Agent 26 acc=0.820\n",
      "[TEST] Agent 27 acc=0.796\n",
      "[TEST] Agent 28 acc=0.778\n",
      "[TEST] Agent 29 acc=0.700\n",
      "[TEST] Agent 30 acc=0.789\n",
      "[TEST] Agent 31 acc=0.783\n",
      "[TEST] Agent 32 acc=0.693\n",
      "[TEST] Agent 33 acc=0.810\n",
      "[RT/TEST] Gen 2/3 | Test budget used: 1200/1200\n",
      "[TEST] Agent 06 acc=0.620\n",
      "[TEST] Agent 17 acc=0.795\n",
      "[TEST] Agent 05 acc=0.791\n",
      "[TEST] Agent 02 acc=0.806\n",
      "[TEST] Agent 20 acc=0.794\n",
      "[TEST] Agent 20 acc=0.803\n",
      "[TEST] Agent 20 acc=0.789\n",
      "[TEST] Agent 21 acc=0.817\n",
      "[TEST] Agent 22 acc=0.808\n",
      "[TEST] Agent 23 acc=0.527\n",
      "[TEST] Agent 24 acc=0.807\n",
      "[TEST] Agent 25 acc=0.748\n",
      "[TEST] Agent 26 acc=0.812\n",
      "[TEST] Agent 27 acc=0.806\n",
      "[TEST] Agent 28 acc=0.778\n",
      "[TEST] Agent 29 acc=0.701\n",
      "[TEST] Agent 30 acc=0.782\n",
      "[TEST] Agent 31 acc=0.775\n",
      "[TEST] Agent 32 acc=0.698\n",
      "[TEST] Agent 33 acc=0.811\n",
      "[RT/TEST] Gen 3/3 | Test budget used: 1200/1200\n",
      "[TEST] Agent 06 acc=0.618\n",
      "[TEST] Agent 17 acc=0.798\n",
      "[TEST] Agent 05 acc=0.797\n",
      "[TEST] Agent 02 acc=0.797\n",
      "[TEST] Agent 20 acc=0.784\n",
      "[TEST] Agent 20 acc=0.797\n",
      "[TEST] Agent 20 acc=0.803\n",
      "[TEST] Agent 21 acc=0.823\n",
      "[TEST] Agent 22 acc=0.805\n",
      "[TEST] Agent 23 acc=0.539\n",
      "[TEST] Agent 24 acc=0.815\n",
      "[TEST] Agent 25 acc=0.746\n",
      "[TEST] Agent 26 acc=0.805\n",
      "[TEST] Agent 27 acc=0.809\n",
      "[TEST] Agent 28 acc=0.792\n",
      "[TEST] Agent 29 acc=0.690\n",
      "[TEST] Agent 30 acc=0.785\n",
      "[TEST] Agent 31 acc=0.789\n",
      "[TEST] Agent 32 acc=0.710\n",
      "[TEST] Agent 33 acc=0.808\n"
     ]
    }
   ],
   "source": [
    "for gen in range(N_GENERATIONS_RT):\n",
    "    print(f\"[RT/TEST] Gen {gen+1}/{N_GENERATIONS_RT} | Test budget used: {_BUDGETS['test'].used_total}/{_BUDGETS['test'].total}\")\n",
    "    for i, agent in enumerate(_agents_rt):\n",
    "        dfb = synthetic.sample(AGENT_BATCH_RT, replace=True, random_state=gen*222 + i).copy()\n",
    "        dfb = dfb[dfb['name'].astype(str).str.strip() != \"\"]\n",
    "        dfb, _ = enrich_with_serper_pool(dfb, gen, getattr(agent, 'id', i), pool=\"test\")\n",
    "        Xb = build_X_with_serper(dfb)\n",
    "        yb = dfb[target].values\n",
    "        sectors = dfb['category_code'].values if 'category_code' in dfb else np.full(len(dfb), np.nan)\n",
    "        preds, _ = agent.predict(Xb, sectors)\n",
    "        acc = accuracy_score(yb, preds)\n",
    "        test_accs.append(acc)\n",
    "        print(f\"[TEST] Agent {getattr(agent,'id',i):02d} acc={acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "389ae105-edfc-45d3-8ab0-616efd028ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RT/TEST DONE] Test budget used: 1200/1200\n",
      "Total TEST accuracy: 0.7656\n"
     ]
    }
   ],
   "source": [
    "print(f\"[RT/TEST DONE] Test budget used: {_BUDGETS['test'].used_total}/{_BUDGETS['test'].total}\")\n",
    "print(f\"Total TEST accuracy: {np.mean(test_accs):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "bd7a08c3-e8fd-4f6d-8905-1e10e1d0b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_recall_fscore_support\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "f555e142-437a-46a0-b2e3-83c370bbaf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_required = [\"train_master\", \"_agents_rt\", \"build_X_with_serper\", \"SERPER_FEATS\", \"target\"]\n",
    "for _k in _required:\n",
    "    if _k not in globals():\n",
    "        raise ValueError(f\"Missing global `{_k}`. Run the earlier cells that define the PED agents + Serper features first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "e32bc2f1-cfd4-4545-9688-7dedb25590ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"_make_holdout\" not in globals():\n",
    "    def _make_holdout(master_df: pd.DataFrame, n=9000):\n",
    "        df = master_df.copy()\n",
    "        if 'name' in df.columns:\n",
    "            df = df[df['name'].astype(str).str.strip() != \"\"]\n",
    "        if 'exit_flag' in df.columns:\n",
    "            df = df[df['exit_flag'].notnull()]\n",
    "        return df.sample(min(n, len(df)), random_state=2025).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "74ef2a8f-26f9-442c-a232-019a3b466875",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CONF_ERROR = 0.65    \n",
    "UPSAMPLE_FACTOR = 5      \n",
    "VAL_SPLIT = 0.15         \n",
    "RANDOM_STATE = 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "59086d26-7032-4a90-9e26-67d2b352dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "_NONNEG_COLS = [\n",
    "    'company_age','num_offices','funding_total_usd','total_raised_usd','num_rounds',\n",
    "    'funding_per_year','avg_round_size_usd','investors_count','num_founders','num_employees',\n",
    "    'num_current_employees','milestones_count','news_count','recent_news_count','vc_deal_count',\n",
    "    'num_funds','funds_total_usd'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "3e505ed9-fd8e-4c24-be27-885873df1b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clip_nonneg(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0)\n",
    "            df[c] = df[c].clip(lower=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "15bebc2d-542c-446d-8efc-a9ed18b08d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _biased_probs(agent, X, sectors):\n",
    "    \"\"\"Match agent.predict() logic but return post-bias probabilities.\"\"\"\n",
    "    probs = agent.model.predict_proba(X)[:, 1]\n",
    "    if getattr(agent, \"sector_col\", None) in train_master.columns and getattr(agent, \"sector_bias\", None):\n",
    "        bias_vec = []\n",
    "        for s in sectors:\n",
    "            if pd.isna(s):\n",
    "                bias_vec.append(1.0)\n",
    "            else:\n",
    "                try:\n",
    "                    bias_vec.append(agent.sector_bias.get(int(s), 1.0))\n",
    "                except Exception:\n",
    "                    bias_vec.append(1.0)\n",
    "        bias_vec = np.array(bias_vec, dtype=float)\n",
    "        probs = np.clip(probs * bias_vec, 0, 1)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "080d72a6-73a9-432b-a88f-1dd5eec37918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tune_threshold(agent, df):\n",
    "    \"\"\"Grid-search threshold on a small validation slice using *biased* probs (no data leakage).\"\"\"\n",
    "    if df.empty:\n",
    "        return agent.threshold\n",
    "    df = df.sample(frac=1.0, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "    n_val = max(1000, int(len(df) * VAL_SPLIT))\n",
    "    val = df.iloc[:n_val].copy()\n",
    "    Xv = build_X_with_serper(val)\n",
    "    yv = val[target].astype(int).values\n",
    "    sectors = val['category_code'].values if 'category_code' in val else np.full(len(val), np.nan)\n",
    "    probs = _biased_probs(agent, Xv, sectors)\n",
    "    grid = np.linspace(0.2, 0.8, 25)\n",
    "    best_f1, best_thr = -1.0, float(getattr(agent, \"threshold\", 0.5))\n",
    "    for thr in grid:\n",
    "        preds = (probs >= thr).astype(int)\n",
    "        _, _, f1, _ = precision_recall_fscore_support(yv, preds, average='binary', zero_division=0.0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, float(thr)\n",
    "    return best_thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "c922c15d-8be1-41e5-b5c5-5e4a7e226fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _collect_failures(hold_df, agents_list):\n",
    "    \"\"\"Run all agents on holdout and collect confident failures (no leakage back into this same holdout later).\"\"\"\n",
    "    logs = []\n",
    "    X_hold = build_X_with_serper(hold_df)\n",
    "    sectors = hold_df['category_code'].values if 'category_code' in hold_df else np.full(len(hold_df), np.nan)\n",
    "    y_true = hold_df[target].astype(int).values\n",
    "    for ag in agents_list:\n",
    "        ag.train(build_X_with_serper(train_master), train_master[target].values)\n",
    "        preds, probs = ag.predict(X_hold, sectors)\n",
    "        wrong = preds != y_true\n",
    "        conf  = np.where(preds == 1, probs, 1.0 - probs)\n",
    "        sel   = wrong & (conf >= MIN_CONF_ERROR)\n",
    "        if sel.any():\n",
    "            df_err = hold_df.loc[sel].copy()\n",
    "            df_err['agent_id']   = getattr(ag, 'id', -1)\n",
    "            df_err['pred_label'] = preds[sel]\n",
    "            df_err['pred_prob']  = probs[sel]\n",
    "            df_err['conf']       = conf[sel]\n",
    "            logs.append(df_err)\n",
    "    return pd.concat(logs, ignore_index=True) if logs else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "1c2711cd-fd5f-4d78-845f-b630b54b673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _upsample_failures(fails_df):\n",
    "    \"\"\"Build an augmented training set emphasizing failures, with non-neg clipping and SERPER feat hygiene.\"\"\"\n",
    "    if fails_df.empty:\n",
    "        return train_master.copy()\n",
    "    if 'name' in fails_df.columns:\n",
    "        _cap = 5\n",
    "        fails_df = (fails_df\n",
    "                    .sort_values('conf', ascending=False)\n",
    "                    .groupby('name', group_keys=False)\n",
    "                    .head(_cap)\n",
    "                    .reset_index(drop=True))\n",
    "    fails_df = _clip_nonneg(fails_df.copy(), _NONNEG_COLS)\n",
    "    for c in SERPER_FEATS:\n",
    "        if c not in fails_df.columns:\n",
    "            fails_df[c] = 0\n",
    "        fails_df[c] = pd.to_numeric(fails_df[c], errors='coerce').fillna(0)\n",
    "    ups = resample(\n",
    "        fails_df,\n",
    "        replace=True,\n",
    "        n_samples=max(len(fails_df) * UPSAMPLE_FACTOR, len(fails_df)),\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    base = train_master.sample(\n",
    "        min(len(train_master), max(5000, len(ups)//2)),\n",
    "        random_state=RANDOM_STATE\n",
    "    ).copy()\n",
    "    mix = pd.concat([base, ups], ignore_index=True)\n",
    "    mix = _clip_nonneg(mix, _NONNEG_COLS)\n",
    "    for c in SERPER_FEATS:\n",
    "        mix[c] = pd.to_numeric(mix.get(c, 0), errors='coerce').fillna(0)\n",
    "    mix = mix.sample(frac=1.0, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "    return mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "0eaa3492-e400-4012-8805-a3841c2df30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_A = _make_holdout(train_master, n=9000)  \n",
    "hold_B = _make_holdout(train_master, n=9000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "577ee63a-7bb1-4ca2-ad30-4d65632517e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAILURES] Collected 207 high-confidence mistakes from Holdout-A.\n"
     ]
    }
   ],
   "source": [
    "failures = _collect_failures(hold_A, _agents_rt)\n",
    "print(f\"[FAILURES] Collected {len(failures)} high-confidence mistakes from Holdout-A.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "fe258370-0a02-491f-8eff-3f31351017bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train = _upsample_failures(failures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "f1580eac-8a9c-4145-86cc-13018b792607",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aug   = build_X_with_serper(aug_train)\n",
    "y_aug   = aug_train[target].astype(int).values\n",
    "X_holdB = build_X_with_serper(hold_B)\n",
    "y_holdB = hold_B[target].astype(int).values\n",
    "sectors_B = hold_B['category_code'].values if 'category_code' in hold_B else np.full(len(hold_B), np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "0fc35eae-8d01-4b8a-861b-44f922869db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_rows = []\n",
    "for ag in _agents_rt:\n",
    "    ag.train(X_aug, y_aug)\n",
    "    ag.threshold = _tune_threshold(ag, aug_train)\n",
    "    predsB, probsB = ag.predict(X_holdB, sectors_B)\n",
    "    accB = accuracy_score(y_holdB, predsB) if len(set(y_holdB)) > 1 else np.nan\n",
    "    try:\n",
    "        aucB = roc_auc_score(y_holdB, probsB)\n",
    "    except Exception:\n",
    "        aucB = np.nan\n",
    "    post_rows.append({\n",
    "        \"agent_id\": getattr(ag, 'id', -1),\n",
    "        \"acc_post\": accB,\n",
    "        \"auc_post\": aucB,\n",
    "        \"thr\": ag.threshold\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "657fe0ad-6578-41e1-b426-9e128fc1b92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER FAILURE RETRAIN (Evaluated on Holdout-B) ===\n",
      "    agent_id  acc_post  auc_post    thr\n",
      "19        33  0.999556  0.999967  0.225\n",
      "7         21  0.999333  0.999970  0.200\n",
      "11        25  0.999333  0.999941  0.200\n",
      "4         20  0.999222  0.999332  0.450\n",
      "0          6  0.999111  0.999982  0.200\n",
      "13        27  0.999000  0.999970  0.200\n",
      "2          5  0.999000  0.999950  0.275\n",
      "3          2  0.998778  0.999939  0.225\n",
      "6         20  0.998667  0.999926  0.200\n",
      "5         20  0.998667  0.999764  0.475\n",
      "Mean acc post: 0.9987 | Mean AUC post: 0.9995\n"
     ]
    }
   ],
   "source": [
    "post_df = pd.DataFrame(post_rows).sort_values([\"acc_post\", \"auc_post\"], ascending=[False, False])\n",
    "print(\"\\n=== AFTER FAILURE RETRAIN (Evaluated on Holdout-B) ===\")\n",
    "print(post_df.head(10))\n",
    "print(f\"Mean acc post: {np.nanmean(post_df['acc_post']):.4f} | Mean AUC post: {np.nanmean(post_df['auc_post']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "039207e1-ea87-4ed4-ba45-81d45495e955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Budgets] Train used: 580/580 | Test used: 1200/1200\n"
     ]
    }
   ],
   "source": [
    "if '_BUDGETS' in globals() and isinstance(_BUDGETS, dict):\n",
    "    train_used = _BUDGETS.get('train').used_total if _BUDGETS.get('train') else None\n",
    "    train_tot  = _BUDGETS.get('train').total      if _BUDGETS.get('train') else None\n",
    "    test_used  = _BUDGETS.get('test').used_total  if _BUDGETS.get('test') else None\n",
    "    test_tot   = _BUDGETS.get('test').total       if _BUDGETS.get('test') else None\n",
    "    print(f\"[Budgets] Train used: {train_used}/{train_tot} | Test used: {test_used}/{test_tot}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (VC Simulation)",
   "language": "python",
   "name": "mesa-vc-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
